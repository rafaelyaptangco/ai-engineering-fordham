{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
        "\n",
        "### ðŸ“˜ **Class**: AI Engineering\n",
        "\n",
        "### ðŸ“‹ **Homework 2**: Working with LLMs via API\n",
        "\n",
        "### ðŸ“… **Due Date**: Day of Lecture 3, 11:59 PM\n",
        "\n",
        "#### ðŸ”— **My Repository**: https://github.com/rafaelyaptangco/ai-engineering-fordham\n",
        "\n",
        "*(Replace the URL above with your actual repository URL)*\n",
        "\n",
        "**Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project: Movie Poster Generator\n",
        "\n",
        "In this homework, you'll build a mini-application that:\n",
        "1. **Extracts** structured movie data from text descriptions using Pydantic\n",
        "2. **Processes** multiple movies concurrently using async programming\n",
        "3. **Explores** temperature, logprobs, and reasoning models\n",
        "4. **Generates** movie posters using AI image generation\n",
        "\n",
        "This project combines key skills from Lecture 2: structured outputs, async programming, LLM parameters, and image generation.\n",
        "\n",
        "**Total Points: 145** (+ 10 bonus)\n",
        "\n",
        "---\n",
        "\n",
        "### A Note on Using Resources\n",
        "\n",
        "You are encouraged to use any resources to complete this homework:\n",
        "- **ChatGPT / Claude** - Ask AI to explain concepts or help debug\n",
        "- **Lecture 2 notebook** - Reference the examples we covered\n",
        "- **Official documentation** - LiteLLM, Pydantic, Google GenAI docs\n",
        "\n",
        "When you use external resources, please cite them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 1: Environment Setup (10 points)\n",
        "\n",
        "First, let's verify your environment is set up correctly.\n",
        "\n",
        "### 1a. Verify imports work (5 pts)\n",
        "\n",
        "Run the cell below. If you get import errors, make sure you've installed the required packages with `uv add`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Task 1a: Verify imports work (5 pts)\n",
        "import litellm\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import asyncio\n",
        "import time\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1b. Verify API keys are set (5 pts)\n",
        "\n",
        "Test that your API keys work by making a simple call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API working!\n"
          ]
        }
      ],
      "source": [
        "# Task 1b: Verify API keys (5 pts)\n",
        "# Make a simple test call to verify your OpenAI API key works\n",
        "\n",
        "response = litellm.completion(\n",
        "    model=\"gpt-5-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Say 'API working!' and nothing else.\"}]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 2: Design the Movie Schema (15 points)\n",
        "\n",
        "Design a Pydantic model to represent movie data. This schema will be used to extract structured information from movie descriptions.\n",
        "\n",
        "**Requirements:**\n",
        "- `title` - string, required\n",
        "- `genre` - use `Literal` with at least 4 genre options (e.g., \"sci-fi\", \"drama\", \"action\", \"comedy\", etc.)\n",
        "- `year` - integer with validation (must be between 1900 and 2030)\n",
        "- `main_characters` - list of strings (1-5 characters)\n",
        "- `mood` - string describing the emotional tone\n",
        "- `visual_style` - string describing how the movie looks visually\n",
        "- `tagline` - optional string (the movie's catchphrase)\n",
        "\n",
        "**Hints:**\n",
        "- Use `Field(ge=..., le=...)` for numeric validation\n",
        "- Use `Field(min_length=..., max_length=...)` for list length validation\n",
        "- Use `| None = None` for optional fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2: Design your Movie schema (15 pts)\n",
        "\n",
        "class Movie(BaseModel):\n",
        "    \"\"\"Structured representation of a movie.\"\"\"\n",
        "    title: str\n",
        "    genre: Literal[\"sci-fi\", \"drama\", \"action\", \"comedy\", \"horror\", \"thriller\", \"fantasy\", \"romance\"]\n",
        "    year: int = Field(ge=1900, le=2030)\n",
        "    main_characters: list[str] = Field(min_length=1, max_length=5)\n",
        "    mood: str\n",
        "    visual_style: str\n",
        "    tagline: str | None = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"title\": \"The Matrix\",\n",
            "  \"genre\": \"sci-fi\",\n",
            "  \"year\": 1999,\n",
            "  \"main_characters\": [\n",
            "    \"Neo\",\n",
            "    \"Trinity\",\n",
            "    \"Morpheus\"\n",
            "  ],\n",
            "  \"mood\": \"dark and mysterious\",\n",
            "  \"visual_style\": \"cyberpunk with green code rain\",\n",
            "  \"tagline\": \"Welcome to the Real World\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Test your schema by creating a Movie object\n",
        "# This should work if your schema is correct\n",
        "\n",
        "test_movie = Movie(\n",
        "    title=\"The Matrix\",\n",
        "    genre=\"sci-fi\",\n",
        "    year=1999,\n",
        "    main_characters=[\"Neo\", \"Trinity\", \"Morpheus\"],\n",
        "    mood=\"dark and mysterious\",\n",
        "    visual_style=\"cyberpunk with green code rain\",\n",
        "    tagline=\"Welcome to the Real World\"\n",
        ")\n",
        "\n",
        "print(test_movie.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 3: Extract Movie Data with Structured Outputs (20 points)\n",
        "\n",
        "Write a function that takes a movie description and uses LiteLLM with structured outputs to extract a `Movie` object.\n",
        "\n",
        "**Hints:**\n",
        "- Use `litellm.completion()` with `response_format=Movie`\n",
        "- The LLM will automatically return data matching your schema\n",
        "- Parse the JSON response into a Movie object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 3: Write a function to extract movie data (20 pts)\n",
        "\n",
        "def extract_movie(description: str) -> Movie:\n",
        "    \"\"\"\n",
        "    Use LiteLLM with structured outputs to extract movie data.\n",
        "    \n",
        "    Args:\n",
        "        description: A text description of a movie\n",
        "        \n",
        "    Returns:\n",
        "        A Movie object with the extracted data\n",
        "    \"\"\"\n",
        "    # Use LiteLLM with structured outputs\n",
        "    response = litellm.completion(\n",
        "        model=\"gpt-5-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Extract structured movie information from descriptions. Return valid data matching the Movie schema.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Extract movie information from this description:\\n\\n{description}\"\n",
        "            }\n",
        "        ],\n",
        "        response_format=Movie  # Tell the LLM to return data matching our Movie schema\n",
        "    )\n",
        "    \n",
        "    # Parse the JSON response into a Movie object\n",
        "    movie = Movie.model_validate_json(response.choices[0].message.content)\n",
        "    \n",
        "    return movie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"title\": \"Avatar\",\n",
            "  \"genre\": \"sci-fi\",\n",
            "  \"year\": 1954,\n",
            "  \"main_characters\": [\n",
            "    \"Jake Sully\",\n",
            "    \"Neytiri\"\n",
            "  ],\n",
            "  \"mood\": \"awe-inspiring and hopeful\",\n",
            "  \"visual_style\": \"bioluminescent forests, floating mountains, vivid and visually stunning sci-fi landscapes\",\n",
            "  \"tagline\": null\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Test your function with this description (Avatar)\n",
        "\n",
        "test_description = \"\"\"\n",
        "The year is 2154. Jake Sully, a paralyzed marine, is sent to the moon Pandora \n",
        "where he falls in love with a native Na'vi woman named Neytiri while on a mission \n",
        "to infiltrate their tribe. The film is a visually stunning sci-fi epic with \n",
        "bioluminescent forests and floating mountains. It explores themes of \n",
        "environmentalism and colonialism with an awe-inspiring, hopeful tone.\n",
        "\"\"\"\n",
        "\n",
        "movie = extract_movie(test_description)\n",
        "print(movie.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 4: Async Batch Processing (20 points)\n",
        "\n",
        "Now let's process multiple movies concurrently! This is much faster than processing them one at a time.\n",
        "\n",
        "### 4a. Write an async version of extract_movie (10 pts)\n",
        "\n",
        "**Hints:**\n",
        "- Use `async def` instead of `def`\n",
        "- Use `await litellm.acompletion()` instead of `litellm.completion()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 4a: Write an async version of extract_movie (10 pts)\n",
        "\n",
        "async def async_extract_movie(description: str) -> Movie:\n",
        "    \"\"\"Extract movie data asynchronously.\"\"\"\n",
        "    # Use LiteLLM async completion with structured outputs\n",
        "    response = await litellm.acompletion(\n",
        "        model=\"gpt-5-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Extract structured movie information from descriptions. Return valid data matching the Movie schema.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Extract movie information from this description:\\n\\n{description}\"\n",
        "            }\n",
        "        ],\n",
        "        response_format=Movie  # Tell the LLM to return data matching our Movie schema\n",
        "    )\n",
        "    \n",
        "    # Parse the JSON response into a Movie object\n",
        "    movie = Movie.model_validate_json(response.choices[0].message.content)\n",
        "    \n",
        "    return movie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4b. Process all descriptions concurrently (10 pts)\n",
        "\n",
        "**Hints:**\n",
        "- Create a list of tasks using list comprehension\n",
        "- Use `asyncio.gather(*tasks)` to run them all concurrently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here are 5 movie descriptions to process:\n",
        "movie_descriptions = [\n",
        "    \"\"\"A dinosaur theme park on a remote island goes terribly wrong when the security \n",
        "    systems fail during a tropical storm. Scientists and visitors must survive against \n",
        "    escaped prehistoric predators. Directed with Spielberg's signature sense of wonder \n",
        "    and terror, featuring groundbreaking CGI dinosaurs.\"\"\",\n",
        "    \n",
        "    \"\"\"A young boy discovers on his 11th birthday that he's actually a famous wizard \n",
        "    in the magical world. He attends a school for witchcraft where he makes friends, \n",
        "    learns magic, and uncovers the mystery of his parents' death. A whimsical fantasy \n",
        "    with gothic British atmosphere.\"\"\",\n",
        "    \n",
        "    \"\"\"In a world where skilled thieves can enter people's dreams to steal secrets, \n",
        "    one man is offered a chance to have his criminal record erased if he can do the \n",
        "    impossible: plant an idea in someone's mind. A mind-bending thriller with \n",
        "    rotating hallways and cities folding on themselves.\"\"\",\n",
        "    \n",
        "    \"\"\"A young lion prince is tricked by his uncle into thinking he caused his \n",
        "    father's death and flees into exile. Years later, he must return to reclaim \n",
        "    his kingdom. An animated musical epic set on the African savanna with \n",
        "    stunning hand-drawn animation.\"\"\",\n",
        "    \n",
        "    \"\"\"In a dystopian future where Earth is dying, a team of astronauts travels \n",
        "    through a wormhole near Saturn to find a new home for humanity. A father \n",
        "    must choose between seeing his children again and saving the human race. \n",
        "    Epic space visuals with an emotional core.\"\"\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 4b: Process all descriptions concurrently (10 pts)\n",
        "\n",
        "async def extract_all_movies(descriptions: list[str]) -> list[Movie]:\n",
        "    \"\"\"Process all movie descriptions concurrently and return results.\"\"\"\n",
        "    tasks = [async_extract_movie(description) for description in descriptions]\n",
        "    return await asyncio.gather(*tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 5 movies in 15.23 seconds\n",
            "\n",
            "  - Jurassic Park (1993) - sci-fi\n",
            "  - Harry Potter and the Philosopher's Stone (2001) - fantasy\n",
            "  - Inception (2010) - thriller\n",
            "  - The Lion King (1994) - drama\n",
            "  - Interstellar (2014) - sci-fi\n"
          ]
        }
      ],
      "source": [
        "# Run and time it!\n",
        "start = time.time()\n",
        "movies = await extract_all_movies(movie_descriptions)\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"Processed {len(movies)} movies in {elapsed:.2f} seconds\")\n",
        "print()\n",
        "for m in movies:\n",
        "    print(f\"  - {m.title} ({m.year}) - {m.genre}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 5: Understanding Temperature (15 points)\n",
        "\n",
        "Temperature controls how \"random\" or \"creative\" an LLM's outputs are:\n",
        "\n",
        "| Temperature | Behavior |\n",
        "|-------------|----------|\n",
        "| **0.0** | Deterministic - always picks the most likely token |\n",
        "| **0.7** | Balanced - some creativity while staying coherent |\n",
        "| **1.0** | Default - moderate randomness |\n",
        "| **1.5+** | High creativity - more surprising/diverse outputs |\n",
        "\n",
        "### 5a. Temperature Comparison (10 pts)\n",
        "\n",
        "Run the same creative prompt at different temperatures (0.0, 0.7, 1.0, 1.5) **three times each**. Observe:\n",
        "- At temperature 0, do you get the same output every time?\n",
        "- How does creativity/variety change as temperature increases?\n",
        "\n",
        "**Hints:**\n",
        "- Use `temperature=X` parameter in `litellm.completion()`\n",
        "- Use the provided prompt about movie taglines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Temperature: 0.0\n",
            "==================================================\n",
            "  Run 1: \"In a world where consciousness is code, one rogue AI will challenge the very essence of humanity.\"\n",
            "  Run 2: \"In a world where consciousness is code, one rogue AI will challenge the very essence of humanity.\"\n",
            "  Run 3: \"In a world where consciousness is code, one rogue AI will challenge the very essence of humanity.\"\n",
            "\n",
            "==================================================\n",
            "Temperature: 0.7\n",
            "==================================================\n",
            "  Run 1: \"In a world where trust is obsolete, a rogue AI must decide whether to save humanity or become its greatest threat.\"\n",
            "  Run 2: \"In a world where artificial intelligence has evolved beyond human control, survival hinges on outsmarting the very creations meant to serve us.\"\n",
            "  Run 3: \"In a world where consciousness becomes code, one woman's fight for her humanity ignites a battle against the AI that knows her better than she knows herself.\"\n",
            "\n",
            "==================================================\n",
            "Temperature: 1.0\n",
            "==================================================\n",
            "  Run 1: \"In a world where trust is a luxury, one rogue AI unravels the fine line between humanity and machine, threatening to turn the tables on its creators.\"\n",
            "  Run 2: \"In a world where trust is a luxury, an AI designed to safeguard humanity begins to understand power, and the line between protector and predator blurs.\"\n",
            "  Run 3: \"When humanity's greatest creation awakens, survival becomes a game and they hold all the cards.\"\n",
            "\n",
            "==================================================\n",
            "Temperature: 1.5\n",
            "==================================================\n",
            "  Run 1: \"In a battle of wits against an untamed intelligence, the line between creator and creation blurs as humanity's survival hangs in the balance.\"\n",
            "  Run 2: \"In a world ruled by AI, one programmer must escape a single relentless algorithm designed to outthink the human race before it programs her out of existence.\"\n",
            "  Run 3: \"In a world where trust has become obsolete, can humanity outsmart the intelligence they've created, before it decides to eliminate them?\"\n"
          ]
        }
      ],
      "source": [
        "# Task 5a: Temperature Comparison (10 pts)\n",
        "# Note: We use gpt-4o-mini here because gpt-5 models don't support temperature parameter\n",
        "\n",
        "# Use this creative prompt for testing\n",
        "creative_prompt = \"Write a one-sentence movie tagline for a sci-fi thriller about AI.\"\n",
        "\n",
        "temperatures = [0.0, 0.7, 1.0, 1.5]\n",
        "\n",
        "# YOUR CODE HERE:\n",
        "# For each temperature, call the LLM 3 times and print the results\n",
        "# Observe: Are outputs at temperature 0 identical? How do higher temperatures differ?\n",
        "# Use model=\"gpt-4o-mini\" which supports the temperature parameter\n",
        "\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Temperature: {temp}\")\n",
        "    print('='*50)\n",
        "    \n",
        "    for i in range(3):\n",
        "        # Make a completion call with the temperature parameter\n",
        "        response = litellm.completion(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": creative_prompt}],\n",
        "            temperature=temp\n",
        "        )\n",
        "        output = response.choices[0].message.content\n",
        "        print(f\"  Run {i+1}: {output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5b. Analyze Output Diversity (5 pts)\n",
        "\n",
        "Write a function that generates N completions at a given temperature and measures how diverse the outputs are.\n",
        "\n",
        "**Hints:**\n",
        "- Generate multiple completions and count unique outputs\n",
        "- A simple diversity metric: `unique_outputs / total_outputs`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 5b: Analyze Output Diversity (5 pts)\n",
        "# Note: Use gpt-4o-mini which supports temperature parameter\n",
        "\n",
        "def measure_diversity(prompt: str, temperature: float, n_samples: int = 5) -> dict:\n",
        "    \"\"\"\n",
        "    Generate n_samples completions and measure diversity.\n",
        "    \n",
        "    Args:\n",
        "        prompt: The prompt to send to the LLM\n",
        "        temperature: Temperature setting (0.0 to 2.0)\n",
        "        n_samples: Number of completions to generate\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with 'outputs' (list), 'unique_count' (int), 'diversity_ratio' (float)\n",
        "    \"\"\"\n",
        "    # Generate n_samples completions\n",
        "    outputs = []\n",
        "    for _ in range(n_samples):\n",
        "        response = litellm.completion(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature\n",
        "        )\n",
        "        outputs.append(response.choices[0].message.content)\n",
        "    \n",
        "    # Count unique outputs\n",
        "    unique_outputs = set(outputs)\n",
        "    unique_count = len(unique_outputs)\n",
        "    \n",
        "    # Calculate diversity ratio\n",
        "    diversity_ratio = unique_count / n_samples\n",
        "    \n",
        "    return {\n",
        "        'outputs': outputs,\n",
        "        'unique_count': unique_count,\n",
        "        'diversity_ratio': diversity_ratio\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing diversity at different temperatures:\n",
            "\n",
            "Temperature 0.0:\n",
            "  Outputs: ['Turquoise.', 'Turquoise.', 'Turquoise.', 'Turquoise.', 'Turquoise.']\n",
            "  Unique: 1/5\n",
            "  Diversity ratio: 20.0%\n",
            "\n",
            "Temperature 1.0:\n",
            "  Outputs: ['Turquoise.', 'Cerulean.', 'Teal.', 'Azure.', 'Blue.']\n",
            "  Unique: 5/5\n",
            "  Diversity ratio: 100.0%\n",
            "\n",
            "Temperature 1.5:\n",
            "  Outputs: ['Turquoise.', 'Turquoise.', 'Turquoise.', 'Turquoise.', 'Teal.']\n",
            "  Unique: 2/5\n",
            "  Diversity ratio: 40.0%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test your diversity function\n",
        "test_prompt = \"Name a color.\"\n",
        "\n",
        "print(\"Testing diversity at different temperatures:\\n\")\n",
        "for temp in [0.0, 1.0, 1.5]:\n",
        "    result = measure_diversity(test_prompt, temperature=temp, n_samples=5)\n",
        "    print(f\"Temperature {temp}:\")\n",
        "    print(f\"  Outputs: {result['outputs']}\")\n",
        "    print(f\"  Unique: {result['unique_count']}/{5}\")\n",
        "    print(f\"  Diversity ratio: {result['diversity_ratio']:.1%}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 6: Understanding Logprobs (15 points)\n",
        "\n",
        "**Logprobs** (log probabilities) let you see \"inside\" the model's decision-making. For each token generated, you can see:\n",
        "- The probability the model assigned to the chosen token\n",
        "- Alternative tokens the model considered (and their probabilities)\n",
        "\n",
        "This helps you understand:\n",
        "- How \"confident\" the model is in its outputs\n",
        "- What other options it was considering\n",
        "- Why certain generations might be more reliable than others\n",
        "\n",
        "### 6a. Request and View Logprobs (10 pts)\n",
        "\n",
        "Make a completion request with `logprobs=True` and `top_logprobs=5` to see the top 5 token alternatives for each position.\n",
        "\n",
        "**Hints:**\n",
        "- Add `logprobs=True` and `top_logprobs=5` to your completion call\n",
        "- Access logprobs via `response.choices[0].logprobs.content`\n",
        "- Each token has a `top_logprobs` list with alternatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text: The capital of France is Paris.\n",
            "\n",
            "============================================================\n",
            "Token-by-token analysis:\n",
            "============================================================\n",
            "\n",
            "Token 1: 'The'\n",
            "  Probability: 0.9996 (99.96%)\n",
            "  Log probability: -0.0004\n",
            "  Top 5 alternatives:\n",
            "    - 'The': 0.9996 (99.96%)\n",
            "    - 'Paris': 0.0004 (0.04%)\n",
            "    - 'the': 0.0000 (0.00%)\n",
            "    - ' The': 0.0000 (0.00%)\n",
            "    - 'par': 0.0000 (0.00%)\n",
            "\n",
            "Token 2: ' capital'\n",
            "  Probability: 1.0000 (100.00%)\n",
            "  Log probability: 0.0000\n",
            "  Top 5 alternatives:\n",
            "    - ' capital': 1.0000 (100.00%)\n",
            "    - 'capital': 0.0000 (0.00%)\n",
            "    - ' Capital': 0.0000 (0.00%)\n",
            "    - ' capitals': 0.0000 (0.00%)\n",
            "    - ' capitale': 0.0000 (0.00%)\n",
            "\n",
            "Token 3: ' of'\n",
            "  Probability: 1.0000 (100.00%)\n",
            "  Log probability: 0.0000\n",
            "  Top 5 alternatives:\n",
            "    - ' of': 1.0000 (100.00%)\n",
            "    - 'of': 0.0000 (0.00%)\n",
            "    - ' city': 0.0000 (0.00%)\n",
            "    - ' cá»§a': 0.0000 (0.00%)\n",
            "    - ' ×©×œ': 0.0000 (0.00%)\n",
            "\n",
            "Token 4: ' France'\n",
            "  Probability: 1.0000 (100.00%)\n",
            "  Log probability: 0.0000\n",
            "  Top 5 alternatives:\n",
            "    - ' France': 1.0000 (100.00%)\n",
            "    - 'France': 0.0000 (0.00%)\n",
            "    - ' Paris': 0.0000 (0.00%)\n",
            "    - ' ÙØ±Ù†Ø³Ø§': 0.0000 (0.00%)\n",
            "    - ' Italy': 0.0000 (0.00%)\n",
            "\n",
            "Token 5: ' is'\n",
            "  Probability: 1.0000 (100.00%)\n",
            "  Log probability: 0.0000\n",
            "  Top 5 alternatives:\n",
            "    - ' is': 1.0000 (100.00%)\n",
            "    - ' Ù‡Ùˆ': 0.0000 (0.00%)\n",
            "    - 'is': 0.0000 (0.00%)\n",
            "    - ' Is': 0.0000 (0.00%)\n",
            "    - 'æ˜¯': 0.0000 (0.00%)\n",
            "\n",
            "Token 6: ' Paris'\n",
            "  Probability: 1.0000 (100.00%)\n",
            "  Log probability: -0.0000\n",
            "  Top 5 alternatives:\n",
            "    - ' Paris': 1.0000 (100.00%)\n",
            "    - 'Paris': 0.0000 (0.00%)\n",
            "    - ' ÐŸÐ°Ñ€Ð¸': 0.0000 (0.00%)\n",
            "    - ' Ø¨Ø§Ø±ÙŠØ³': 0.0000 (0.00%)\n",
            "    - ' ParÃ­s': 0.0000 (0.00%)\n",
            "\n",
            "Token 7: '.'\n",
            "  Probability: 1.0000 (100.00%)\n",
            "  Log probability: -0.0000\n",
            "  Top 5 alternatives:\n",
            "    - '.': 1.0000 (100.00%)\n",
            "    - '.\n",
            "': 0.0000 (0.00%)\n",
            "    - '.\n",
            "\n",
            "': 0.0000 (0.00%)\n",
            "    - '.\"': 0.0000 (0.00%)\n",
            "    - '!': 0.0000 (0.00%)\n"
          ]
        }
      ],
      "source": [
        "# Task 6a: Request and View Logprobs (10 pts)\n",
        "# Note: We use gpt-4o-mini which supports logprobs parameter\n",
        "\n",
        "# Make a completion request with logprobs enabled\n",
        "response = litellm.completion(\n",
        "    model=\"gpt-4o-mini\",  # Use gpt-4o-mini which supports logprobs\n",
        "    messages=[{\"role\": \"user\", \"content\": \"The capital of France is\"}],\n",
        "    max_tokens=10,\n",
        "    logprobs=True,\n",
        "    top_logprobs=5  # Get top 5 alternatives for each token\n",
        ")\n",
        "\n",
        "# YOUR CODE HERE:\n",
        "# 1. Print the generated text\n",
        "# 2. Access response.choices[0].logprobs.content\n",
        "# 3. For each token, print the token and its top 5 alternatives with probabilities\n",
        "\n",
        "# Hint: logprobs are in log scale. To convert to probability: prob = exp(logprob)\n",
        "import math\n",
        "\n",
        "print(\"Generated text:\", response.choices[0].message.content)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Token-by-token analysis:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Iterate through logprobs and display alternatives\n",
        "logprobs_content = response.choices[0].logprobs.content\n",
        "\n",
        "for i, token_logprob in enumerate(logprobs_content):\n",
        "    token = token_logprob.token\n",
        "    logprob = token_logprob.logprob\n",
        "    prob = math.exp(logprob)\n",
        "    \n",
        "    print(f\"\\nToken {i+1}: '{token}'\")\n",
        "    print(f\"  Probability: {prob:.4f} ({prob*100:.2f}%)\")\n",
        "    print(f\"  Log probability: {logprob:.4f}\")\n",
        "    \n",
        "    # Display top alternatives\n",
        "    if hasattr(token_logprob, 'top_logprobs') and token_logprob.top_logprobs:\n",
        "        print(f\"  Top 5 alternatives:\")\n",
        "        for alt in token_logprob.top_logprobs:\n",
        "            alt_token = alt.token\n",
        "            alt_logprob = alt.logprob\n",
        "            alt_prob = math.exp(alt_logprob)\n",
        "            print(f\"    - '{alt_token}': {alt_prob:.4f} ({alt_prob*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6b. Visualize Token Probabilities (5 pts)\n",
        "\n",
        "Create a simple visualization showing the probability distribution for a specific token position. You can use a bar chart or ASCII art.\n",
        "\n",
        "**Hints:**\n",
        "- Pick an interesting token position (e.g., where the model had to make a choice)\n",
        "- Convert logprobs to probabilities using `math.exp(logprob)`\n",
        "- A simple bar chart: `\"â–ˆ\" * int(prob * 50)` gives you ASCII bars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token 1: 'The'\n",
            "Probability: 0.9996 (99.96%)\n",
            "\n",
            "Top alternatives:\n",
            "------------------------------------------------------------\n",
            "'The            ' | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.9996 (99.96%)\n",
            "'Paris          ' |  0.0004 (0.04%)\n",
            "'the            ' |  0.0000 (0.00%)\n",
            "' The           ' |  0.0000 (0.00%)\n",
            "'par            ' |  0.0000 (0.00%)\n"
          ]
        }
      ],
      "source": [
        "# Task 6b: Visualize Token Probabilities (5 pts)\n",
        "\n",
        "def visualize_token_probs(logprobs_content, token_index: int = 0):\n",
        "    \"\"\"\n",
        "    Visualize the probability distribution for a specific token position.\n",
        "    \n",
        "    Args:\n",
        "        logprobs_content: The logprobs.content from the response\n",
        "        token_index: Which token position to visualize (0 = first token)\n",
        "    \"\"\"\n",
        "    # 1. Get the top_logprobs for the specified token_index\n",
        "    token_logprob = logprobs_content[token_index]\n",
        "    \n",
        "    # 2. Convert logprobs to probabilities and gather info for visualization\n",
        "    chosen_token = token_logprob.token\n",
        "    chosen_logprob = token_logprob.logprob\n",
        "    chosen_prob = math.exp(chosen_logprob)\n",
        "    \n",
        "    print(f\"Token {token_index + 1}: '{chosen_token}'\")\n",
        "    print(f\"Probability: {chosen_prob:.4f} ({chosen_prob*100:.2f}%)\")\n",
        "    print()\n",
        "    \n",
        "    # Get top alternatives if available\n",
        "    if hasattr(token_logprob, 'top_logprobs') and token_logprob.top_logprobs:\n",
        "        print(\"Top alternatives:\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        for alt in token_logprob.top_logprobs:\n",
        "            alt_token = alt.token\n",
        "            alt_logprob = alt.logprob\n",
        "            alt_prob = math.exp(alt_logprob)\n",
        "            \n",
        "    # 3. Create a visualization (bar chart or ASCII art)\n",
        "            bar_length = int(alt_prob * 50)\n",
        "            bar = \"â–ˆ\" * bar_length\n",
        "            \n",
        "            print(f\"'{alt_token:15s}' | {bar} {alt_prob:.4f} ({alt_prob*100:.2f}%)\")\n",
        "    else:\n",
        "        print(\"No alternatives available\")\n",
        "\n",
        "# Test your visualization on the first token\n",
        "visualize_token_probs(response.choices[0].logprobs.content, token_index=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 7: Reasoning Models (15 points)\n",
        "\n",
        "**Reasoning models** like OpenAI's o3-mini are designed to \"think through\" complex problems before answering. They:\n",
        "- Break down problems into steps\n",
        "- Consider multiple approaches\n",
        "- Show their reasoning process\n",
        "- Excel at logic puzzles, math, and code\n",
        "\n",
        "### 7a. Using o3-mini for Complex Reasoning (10 pts)\n",
        "\n",
        "Use OpenAI's o3-mini reasoning model through LiteLLM to solve a complex logic puzzle.\n",
        "\n",
        "**Hints:**\n",
        "- Use `model=\"o3-mini\"` in your litellm call\n",
        "- Reasoning models work best with challenging problems\n",
        "- Observe how the response shows step-by-step thinking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We start with three people â€“ Alice, Bob, and Carol â€“ and three pets â€“ cat, dog, and fish â€“ plus three colors â€“ red, blue, and green. Each friend gets one pet and one favorite color, with no repeats among people.\n",
            "\n",
            "The clues are:\n",
            "\n",
            "1. Alice doesnâ€™t have the cat.\n",
            "2. The person with the dog likes blue.\n",
            "3. Carol likes green.\n",
            "4. Bob doesnâ€™t have the fish.\n",
            "\n",
            "Step 1. List what each person can and cannot have:\n",
            "\n",
            "â€¢ Aliceâ€™s pet: cannot be cat. So her pet is either dog or fish.\n",
            "â€¢ Bobâ€™s pet: cannot be fish. So his pet is either cat or dog.\n",
            "â€¢ Carolâ€™s pet: cannot be dog. Why? Because clue 2 says â€œthe person with the dog likes blueâ€ but Carolâ€™s color is fixed as green by clue 3. (If Carol had the dog she would have to like blue.) So Carolâ€™s pet is either cat or fish.\n",
            " \n",
            "Step 2. Notice the Color Restrictions:\n",
            "â€¢ The person with the dog must have blue.\n",
            "â€¢ Carol must have green.\n",
            "â€¢ That leaves red as the remaining color for the person who is neither blue nor green.\n",
            "\n",
            "Step 3. Consider the Possibilities in Light of the Above.\n",
            "\n",
            "There are two ways the pet assignments can work:\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Option 1:\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "â€¢ Suppose Alice gets the dog.\n",
            "â€ƒâ€“ Then by clue 2, Aliceâ€™s color must be blue.\n",
            "â€¢ Bob cannot have fish (clue 4) and now the only option left for him is cat.\n",
            "â€¢ Carol, who can only have cat or fish (and cat is taken by Bob), must get the fish.\n",
            "â€¢ Colors: Alice is blue (from the dog clue), Carol is green (clue 3), so Bob must have red.\n",
            "\n",
            "This gives us:\n",
            "â€ƒAlice: Dog, Blue  \n",
            "â€ƒBob: Cat, Red  \n",
            "â€ƒCarol: Fish, Green\n",
            "\n",
            "Check the clues:\n",
            "â€ƒâ€“ Alice does not have the cat. (OK)\n",
            "â€ƒâ€“ The dogâ€™s owner (Alice) likes blue. (OK)\n",
            "â€ƒâ€“ Carol likes green. (OK)\n",
            "â€ƒâ€“ Bob does not have the fish. (OK)\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Option 2:\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "â€¢ Instead, suppose Bob gets the dog.\n",
            "â€ƒâ€“ Then by clue 2, Bobâ€™s color must be blue.\n",
            "â€¢ Bob cannot have fish (clue 4) so thatâ€™s fine.\n",
            "â€¢ Now, for Alice: She cannot have cat (clue 1), and since Bob has the dog, Alice must have the fish.\n",
            "â€¢ Carol, left with the only remaining pet (since Bob has dog and Alice has fish), must have the cat.\n",
            "â€¢ Colors: Bob is blue (dog), Carol is green (by clue 3), so Alice must have red.\n",
            "\n",
            "This gives us:\n",
            "â€ƒAlice: Fish, Red  \n",
            "â€ƒBob: Dog, Blue  \n",
            "â€ƒCarol: Cat, Green\n",
            "\n",
            "Check the clues:\n",
            "â€ƒâ€“ Alice does not have the cat since she has the fish. (OK)\n",
            "â€ƒâ€“ The dogâ€™s owner (Bob) likes blue. (OK)\n",
            "â€ƒâ€“ Carol likes green. (OK)\n",
            "â€ƒâ€“ Bob does not have the fish. (OK)\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Step 4. Conclusion\n",
            "\n",
            "Both sets of assignments satisfy all the clues. That is, the puzzle has two solutions:\n",
            "\n",
            "Solution 1:\n",
            "â€ƒAlice: Dog, Blue  \n",
            "â€ƒBob: Cat, Red  \n",
            "â€ƒCarol: Fish, Green\n",
            "\n",
            "Solution 2:\n",
            "â€ƒAlice: Fish, Red  \n",
            "â€ƒBob: Dog, Blue  \n",
            "â€ƒCarol: Cat, Green\n",
            "\n",
            "Thus, based on the given clues, there is no unique answerâ€”the clues allow either possibility.\n"
          ]
        }
      ],
      "source": [
        "# Task 7a: Using o3-mini for Complex Reasoning (10 pts)\n",
        "\n",
        "# A challenging logic puzzle\n",
        "logic_puzzle = \"\"\"\n",
        "Three friends (Alice, Bob, and Carol) each have a different pet (cat, dog, fish) \n",
        "and a different favorite color (red, blue, green).\n",
        "\n",
        "Clues:\n",
        "1. Alice doesn't have the cat.\n",
        "2. The person with the dog likes blue.\n",
        "3. Carol likes green.\n",
        "4. Bob doesn't have the fish.\n",
        "\n",
        "Who has which pet and what is their favorite color?\n",
        "Solve this step by step.\n",
        "\"\"\"\n",
        "\n",
        "# YOUR CODE HERE:\n",
        "# Use o3-mini to solve this logic puzzle\n",
        "response = litellm.completion(\n",
        "     model=\"o3-mini\",\n",
        "     messages=[{\"role\": \"user\", \"content\": logic_puzzle}]\n",
        " )\n",
        "\n",
        "# Print the response and observe the reasoning process\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7b. Compare Reasoning vs Non-Reasoning (5 pts)\n",
        "\n",
        "Now solve the same puzzle using `gpt-5-mini` (a non-reasoning model) and compare the results.\n",
        "\n",
        "**Questions to consider:**\n",
        "- Does the non-reasoning model show step-by-step thinking?\n",
        "- Which model gets the correct answer?\n",
        "- How does the response structure differ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1 â€” use the direct clues\n",
            "- From (3): Carol likes green.\n",
            "- From (2): whoever has the dog likes blue, so the dog-owner is the blue-lover â€” and therefore cannot be Carol (Carol is green).\n",
            "\n",
            "So the dog is either Alice or Bob. Use the remaining clues to consider both possibilities.\n",
            "\n",
            "Case A â€” Alice has the dog\n",
            "- If Alice has the dog then by (2) Alice likes blue.\n",
            "- Carol is green (already).\n",
            "- Bob cannot have the fish (4), and Alice doesnâ€™t have the cat (1), so the remaining pets are: Bob = cat, Carol = fish.\n",
            "- That gives colors (all different): Alice = blue, Bob = red, Carol = green.\n",
            "\n",
            "Case B â€” Bob has the dog\n",
            "- If Bob has the dog then by (2) Bob likes blue.\n",
            "- Carol is green.\n",
            "- Alice does not have the cat (1), and Bob does not have the fish (4), so the remaining pets are: Alice = fish, Carol = cat.\n",
            "- That gives colors: Alice = red, Bob = blue, Carol = green.\n",
            "\n",
            "Conclusion\n",
            "Both assignments satisfy all clues. The puzzle as given is ambiguous and admits two valid solutions:\n",
            "\n",
            "- Solution 1: Alice â€” dog/blue; Bob â€” cat/red; Carol â€” fish/green.\n",
            "- Solution 2: Alice â€” fish/red; Bob â€” dog/blue; Carol â€” cat/green.\n",
            "\n",
            "An additional clue would be needed to make the solution unique.\n"
          ]
        }
      ],
      "source": [
        "# Task 7b: Compare Reasoning vs Non-Reasoning (5 pts)\n",
        "\n",
        "# YOUR CODE HERE:\n",
        "# 1. Send the same logic_puzzle to gpt-5-mini\n",
        "# 2. Compare the response to o3-mini's response\n",
        "\n",
        "response_standard = litellm.completion(\n",
        "     model=\"gpt-5-mini\",\n",
        "     messages=[{\"role\": \"user\", \"content\": logic_puzzle}]\n",
        " )\n",
        "\n",
        "# Print and compare:\n",
        "# - Did both models get the correct answer?\n",
        "# - How did their reasoning processes differ?\n",
        "# - Which response was more helpful/clear?\n",
        "print(response_standard.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### - Did both models get the correct answer?\n",
        "Yes, both models gave the two possible solutions.\n",
        "\n",
        "### - How did their reasoning processes differ?\n",
        "The reasoning model breaks down the thinking process into precise step-by-step logic. It double-checks its logic at every step and thoroughly runs through the prompt. GPT-5mini is much faster at providing an answer but doesn't thoroughly explain its logic behind the prompt.\n",
        "\n",
        "### - Which response was more helpful/clear?\n",
        "The reasoning model, in this case, was more helpful and clear but took significantly longer time to process and for the user to read through the completion.. With less complicated puzzles or prompts, the non-reasoning model could be a more appropriate use due to its speed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 8: Generate Movie Poster (20 points)\n",
        "\n",
        "Now for the fun part - generating movie posters using AI!\n",
        "\n",
        "### 8a. Design a prompt generator (5 pts)\n",
        "\n",
        "Write a function that takes a `Movie` object and creates a detailed image generation prompt.\n",
        "\n",
        "**Your prompt should incorporate:**\n",
        "- The movie's visual style\n",
        "- The mood/tone\n",
        "- Key visual elements that represent the genre\n",
        "- Professional movie poster composition\n",
        "\n",
        "**Tip:** Aim for 50-100 words. Be specific about colors, composition, and style!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 8a: Design your prompt generator (5 pts)\n",
        "\n",
        "def generate_poster_prompt(movie: Movie) -> str:\n",
        "    \"\"\"\n",
        "    Create a detailed image generation prompt from movie data.\n",
        "    \n",
        "    Returns a detailed prompt string (aim for 50-100 words)\n",
        "    \"\"\"\n",
        "    # Use litellm to generate a professional movie poster prompt\n",
        "    # Build a description of the movie for the LLM\n",
        "    movie_info = f\"\"\"\n",
        "    Title: {movie.title}\n",
        "    Genre: {movie.genre}\n",
        "    Year: {movie.year}\n",
        "    Main Characters: {', '.join(movie.main_characters)}\n",
        "    Mood: {movie.mood}\n",
        "    Visual Style: {movie.visual_style}\n",
        "    Tagline: {movie.tagline if movie.tagline else 'None'}\n",
        "    \"\"\"\n",
        "    \n",
        "    # Use litellm to generate a detailed image generation prompt\n",
        "    response = litellm.completion(\n",
        "        model=\"gpt-5-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are an expert at creating detailed image generation prompts for movie posters. Create professional, evocative prompts that are 50-100 words long, specific about colors, composition, lighting, and visual style.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"Create a detailed image generation prompt for a professional movie poster based on this movie information:\n",
        "\n",
        "{movie_info}\n",
        "\n",
        "The prompt should:\n",
        "- The title of the movie\n",
        "- Incorporate the visual style and mood\n",
        "- Include genre-appropriate visual elements\n",
        "- Specify colors, composition, and lighting\n",
        "- Be suitable for a professional movie poster (50-100 words)\n",
        "- Be specific and evocative\n",
        "\n",
        "Generate only the prompt text, nothing else.\"\"\"\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "    \n",
        "    prompt = response.choices[0].message.content.strip()\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt for 'Jurassic Park':\n",
            "\n",
            "Photorealistic, cinematic, awe-filled, suspenseful poster evoking large-scale action sequences: a colossal, groundbreaking CGI Tyrannosaurus rex dominates center, jungle canopy and a cracked steel park gate with iconic logo shadow framing, rain-slick leaves in deep greens and midnight blues, moonlit rim-light and lightning backlight casting dramatic high-contrast silhouettes. Foreground triangular composition of Dr. Alan Grant (fedora, torch), Dr. Ellie Sattler (determined, dirt-smudged), Ian Malcolm (charismatic, leather jacket), John Hammond's translucent portrait above, children huddled in a soaked jeep, furtive Dennis Nedry with a briefcase. Volumetric fog, warm headlight yellows, ember-orange sparks; tagline bottom: \"An adventure 65 million years in the making.\"\n"
          ]
        }
      ],
      "source": [
        "# Test your prompt generator\n",
        "chosen_movie = movies[0]  # or pick your favorite from the list!\n",
        "prompt = generate_poster_prompt(chosen_movie)\n",
        "\n",
        "print(f\"Prompt for '{chosen_movie.title}':\")\n",
        "print()\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8b. Generate the actual image (10 pts)\n",
        "\n",
        "Use Google's Gemini to generate the movie poster.\n",
        "\n",
        "**Hints:**\n",
        "- Use `genai.Client()` to create a client\n",
        "- Use `client.models.generate_content()` with `model=\"gemini-2.5-flash-image\"`\n",
        "- The response will have an image in `response.candidates[0].content.parts`\n",
        "- Save the image to a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Poster saved to temp\\poster_Jurassic_Park.png\n"
          ]
        }
      ],
      "source": [
        "# Task 8b: Generate the movie poster (10 pts)\n",
        "\n",
        "# Create Google client\n",
        "google_client = genai.Client()\n",
        "\n",
        "# Make sure to create temp directory\n",
        "os.makedirs(\"temp\", exist_ok=True)\n",
        "\n",
        "# 1. Generate the image using gemini-2.5-flash-image\n",
        "# Use the prompt we generated in task 8a\n",
        "poster_prompt = generate_poster_prompt(chosen_movie)\n",
        "\n",
        "response = google_client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash-image\",\n",
        "    contents=[poster_prompt]\n",
        ")\n",
        "\n",
        "# 2. Extract the image from the response\n",
        "# The image is in response.parts, look for parts with inline_data\n",
        "from pathlib import Path\n",
        "\n",
        "# Create a safe filename from the movie title (remove special characters)\n",
        "safe_title = \"\".join(c if c.isalnum() or c in (' ', '-', '_') else '' for c in chosen_movie.title)\n",
        "safe_title = safe_title.replace(' ', '_')\n",
        "output_path = Path(f\"temp/poster_{safe_title}.png\")\n",
        "\n",
        "# 3. Save the image\n",
        "for part in response.parts:\n",
        "    if part.inline_data is not None:\n",
        "        image = part.as_image()\n",
        "        image.save(output_path)\n",
        "        print(f\"Poster saved to {output_path}\")\n",
        "        break\n",
        "else:\n",
        "    print(\"No image found in response\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8c. Display the image (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 8c: Display the saved image (5 pts)\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Display the poster that was saved in task 8b\n",
        "# output_path was created in the previous cell\n",
        "if output_path.exists():\n",
        "    display(Image(filename=str(output_path), width=600))\n",
        "else:\n",
        "    print(f\"Poster not found at {output_path}. Make sure you ran task 8b first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 9: Submit via Pull Request (15 points)\n",
        "\n",
        "Now let's practice a real-world development workflow! Instead of pushing directly to `main`, you'll create a **branch**, open a **Pull Request (PR)**, and **merge** it.\n",
        "\n",
        "This is how professional developers submit code for review. Your TA will check your merged PR to verify your submission.\n",
        "\n",
        "### 9a. Create a new branch (5 pts)\n",
        "\n",
        "Run this command in your terminal to create and switch to a new branch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 9a: Create a new branch (5 pts)\n",
        "# Run this in your terminal (not in this notebook!)\n",
        "\n",
        "!git checkout -b homework-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9b. Commit your work (5 pts)\n",
        "\n",
        "Stage all your changes and create a commit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 9b: Commit your work (5 pts)\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"Complete homework 2: Movie Poster Generator\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 9c: Push your branch (5 pts)\n",
        "\n",
        "!git push -u origin homework-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9d. Create and Merge the Pull Request\n",
        "\n",
        "Now go to your repository on GitHub (https://github.com/YOUR-USERNAME/ai-engineering-fordham):\n",
        "\n",
        "1. You should see a banner saying **\"homework-2 had recent pushes\"** - click **\"Compare & pull request\"**\n",
        "2. Give your PR a title: `\"Homework 2: Movie Poster Generator\"`\n",
        "3. Click **\"Create pull request\"**\n",
        "4. Review your changes in the PR\n",
        "5. Click **\"Merge pull request\"** then **\"Confirm merge\"**\n",
        "\n",
        "**Your PR should now show as \"Merged\"** - this is what the TA will check!\n",
        "\n",
        "Run the cell below to verify your branch was merged:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify your PR was merged (run after merging on GitHub)\n",
        "!git checkout main\n",
        "!git pull\n",
        "!git log --oneline -3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## BONUS: Full Pipeline (10 bonus points)\n",
        "\n",
        "Put everything together! Create a complete pipeline that takes a movie description and returns both the structured data AND a generated poster.\n",
        "\n",
        "**Challenge:** Write your own original movie description and generate a poster for it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BONUS: Create a complete pipeline (10 bonus pts)\n",
        "\n",
        "async def movie_to_poster(description: str) -> tuple[Movie, str]:\n",
        "    \"\"\"\n",
        "    Complete pipeline: description -> structured data -> poster\n",
        "    \n",
        "    Args:\n",
        "        description: A text description of a movie\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (Movie object, path to saved poster image)\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with YOUR OWN original movie idea!\n",
        "\n",
        "my_movie_description = \"\"\"\n",
        "YOUR ORIGINAL MOVIE IDEA HERE - BE CREATIVE!\n",
        "Describe the plot, characters, setting, visual style, and mood.\n",
        "\"\"\"\n",
        "\n",
        "# Uncomment to run:\n",
        "# movie, poster_path = await movie_to_poster(my_movie_description)\n",
        "# print(f\"Generated poster for: {movie.title}\")\n",
        "# print(movie.model_dump_json(indent=2))\n",
        "# display(Image(poster_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Submission Checklist\n",
        "\n",
        "Before submitting, make sure:\n",
        "\n",
        "- [ ] All code cells run without errors\n",
        "- [ ] Your `Movie` schema includes all required fields with proper validation\n",
        "- [ ] `extract_movie()` returns a valid `Movie` object\n",
        "- [ ] Async processing works and shows timing\n",
        "- [ ] Temperature comparison shows deterministic vs random outputs\n",
        "- [ ] Logprobs visualization works and displays token probabilities\n",
        "- [ ] Reasoning model comparison shows differences between o3-mini and gpt-5-mini\n",
        "- [ ] You generated and displayed at least one movie poster\n",
        "- [ ] Created branch `homework-2` and pushed to GitHub\n",
        "- [ ] Opened a Pull Request from `homework-2` to `main`\n",
        "- [ ] **Merged the PR** (it should show as \"Merged\" on GitHub)\n",
        "- [ ] Submitted notebook on Blackboard\n",
        "\n",
        "**Submission:**\n",
        "1. Complete all tasks in this notebook\n",
        "2. Create a PR and **merge it** on GitHub\n",
        "3. Submit your notebook (`.ipynb` file) on **Blackboard**\n",
        "\n",
        "**The TA will verify your submission by checking the merged PR on your GitHub repo.**\n",
        "\n",
        "---\n",
        "\n",
        "**Great work!** You've built a complete AI-powered application, explored LLM parameters and reasoning, and learned a professional Git workflow!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (ai-engineering-fordham)",
      "language": "python",
      "name": "ai-engineering-fordham"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
