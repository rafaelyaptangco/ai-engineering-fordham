{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
        "\n",
        "### ðŸ“˜ **Class**: AI Engineering\n",
        "\n",
        "### ðŸ“‹ **Topic**: You Can Just Build Things\n",
        "\n",
        "ðŸš« **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Welcome!\n",
        "\n",
        "In our firstfour lectures, we've covered how\n",
        "1. We can call LLMs via APIs and get structured responses\n",
        "2. We can build lexical search with BM25\n",
        "3. We can build semantic search with embeddings\n",
        "4. We can combine lexical and semantic search into hybrid search\n",
        "\n",
        "Today you will put it all together by building a Retrieval Augmented Generation (RAG) system.\n",
        "- This is a question-answering bot that can answer questions about Fordham University\n",
        "- You will use real data scraped from the Fordham website.\n",
        "\n",
        "\n",
        "Your RAG pipeline will look like this:\n",
        "\n",
        "```\n",
        "User Question\n",
        "     â†“\n",
        "1. RETRIEVE: Find relevant documents (search!)\n",
        "     â†“\n",
        "2. AUGMENT: Stuff those documents into a prompt\n",
        "     â†“\n",
        "3. GENERATE: Ask an LLM to answer using the context\n",
        "     â†“\n",
        "Answer\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 1. Look at your data\n",
        "\n",
        "In `data/fordham-website.zip` you'll find **~9,500 Markdown files** scraped from Fordham's website. Each file is one page â€” admissions info, program descriptions, faculty pages, financial aid, campus life, and more.\n",
        "\n",
        "Your task: **look at the data**\n",
        "- The first step in any AI engineering or data science project should always be to familiarize yourself with the data.\n",
        "- I cannot stress this enough.. without this step, it's hard to build anything useful.\n",
        "\n",
        "Tips:\n",
        "- Unzip the archive and look at some of the files. \n",
        "- Open a few in a text editor. \n",
        "- Get a feel for what you're working with.\n",
        "- The first line of every file is always the **URL** of the page it was scraped from. The rest is the page content converted to Markdown. Here's an example â€” `gabelli-school-of-business_veterans.md`:\n",
        "\n",
        "```markdown\n",
        "https://www.fordham.edu/gabelli-school-of-business/veterans\n",
        "\n",
        "# Military Veterans & Active Duty Members of the Military\n",
        "\n",
        "## Transform Your Knowledge & Skills Into a Business Career for the Future\n",
        "\n",
        "As a veteran or an active duty member of the United States Armed Services,\n",
        "you have gained or are currently acquiring the invaluable organizational,\n",
        "leadership, analytics, and technical knowledge and skills that hiring\n",
        "managers seek. These transferrable skills provide a major advantage in\n",
        "emerging, business-related industries where innovation, a global mind-set,\n",
        "and the ability to lead individuals and teams in the continuously evolving\n",
        "work environment, are critical for success.\n",
        "\n",
        "By completing a graduate or undergraduate business degree at the Gabelli\n",
        "School of Business, you can prepare for a lifelong career in some of\n",
        "today's fastest-growing fields. ...\n",
        "\n",
        "### Study at a Top-Ranked, Military-Friendly University\n",
        "\n",
        "The Gabelli School of Business is part of Fordham University, the only\n",
        "New York City university to be among those ranked \"Best for Vets\" by\n",
        "Military Times. ...\n",
        "\n",
        "### Learn How the Yellow Ribbon Program Works\n",
        "\n",
        "The Yellow Ribbon GI Education Enhancement Program, or the Yellow Ribbon\n",
        "Program, is a part of the Post-9/11 Veterans Educational Assistance Act\n",
        "of 2008. ...\n",
        "```\n",
        "\n",
        "The filenames mirror the URL structure â€” underscores replace path separators (e.g. `gabelli-school-of-business_veterans.md` came from `/gabelli-school-of-business/veterans`). Some files are short (a few lines), others are quite long.\n",
        "\n",
        "- Once you've looked around, load the files into Python. Python's built-in `zipfile` module can read zip archives without extracting to disk. Load them into a list of dictionaries or a DataFrame with at least two fields: the filename (or a clean page name) and the content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9560,\n",
              " [{'page_name': 'Index',\n",
              "   'content': 'https://www.fordham.edu/\\n\\n## Doing Good That Becomes Greater As The Jesuit University of New York\\n\\nWeâ€™re located in New York Cityâ€”driven by our Jesuit values and tackling todayâ€™s most pressing issues at the center of the world stage.\\n\\n## We Are Leaders, Dreamers, Achievers, And Doers\\n\\nWith sound hearts, strong minds, and the wisdom to take charge, generations of Rams have found what they have needed to growâ€”the opportunities, connections, and support of this community.\\n\\n## From Winding Elms to Bustling City Blocks\\n\\nWith residential campuses in the Bronx and Manhattan, as well as campuses in Westchester and London, Fordham provides endless opportunities to start working toward your career and building the life you want.\\n\\n\\n## Weâ€™re Drawn to Where Weâ€™re Needed Most\\n\\nExplore how our values come to life: how Fordhamâ€™s students, faculty, and alumni contribute to society and make lives better.\\n\\n**Notice of Nondiscriminatory Policy:**\\n\\nFordham University admits students of any race, color, national and ethnic origin to all the rights, privileges, programs, and activities generally accorded or made available to students at the school. It does not discriminate on the basis of race, color, national and ethnic origin in administration of its educational policies, admissions policies, scholarship and loan programs, and athletic and other school-administered programs.\\n\\nFordham University is an Equal Opportunity Employer committed to the principle of equal opportunity in education and employment in compliance with Title IX of the Education Amendments of 1972, Section 504 of the Rehabilitation Act of 1973, Title VI and Title VII of the Civil Rights Act of 1964, the Age Discrimination Act of 1975, the Americans with Disabilities Act of 1990, the Violence Against Women Act, and other federal, state, and local laws.'},\n",
              "  {'page_name': 'Research',\n",
              "   'content': 'https://www.fordham.edu/research\\n\\nUncovering the Limitless Possibilities of Science\\n\\nThe Fordham Undergraduate Research Journal, or FURJ, is a student-run journal that features high quality, peer-reviewed, original research.\\n\\nThe Graduate School of Arts and Sciences is pleased to support the research efforts of students. Find the right grant for your graduate level research.\\n\\nJoin us for the upcoming Annual Undergraduate Research Symposium.\\n\\nFordham and four of our neighbors in the Bronx have created a new model for scientific research, education, and community engagement.\\n\\nThis beautiful up-state estate was donated to Fordham for field research in the biological sciences.\\n\\nThe Greyston Bakery in Yonkers, New York, has a management philosophy that employees call life-changing. Itâ€™s based on trust, as seen in the open hiring processâ€”no resumes or interviews required. â€¦\\n\\nRead The Full Story Arrow right icon\\n\\nEven as the rate of inflation subsided in 2023, the amount of stress it was causing in the U.S. population actually ticked upâ€”indicating that researchers need to pay more attention â€¦\\n\\nWith online threats on the rise around the world, one Fordham professor is working on a potentially revolutionary way to head them off and stay one step ahead of the â€¦\\n\\nIsaie Dougnon, Ph.D., an associate professor of French and Francophone studies and international humanitarian affairs, has spent the last few months running a research project that hits close to homeâ€” â€¦\\n\\nA new generation of students at the Bronx Jewish History Project is getting help from those with experienceâ€”the groupâ€™s student co-founder and a professional historian. BJHP is a Fordham research â€¦\\n\\nA Fordham graduate studentâ€™s research is impacting policy around food benefits for young people. This fall in Arizona, advocates used a research report from Alexander Meyer, a Fordham student in â€¦\\n\\nA researcher seeks to advance regenerative medicine After graduating from Fordham College at Rose Hill in 2022 with a degree in integrative neuroscience, Diego Perez landed a spot in the â€¦\\n\\nAn Engineering Physics Majors Launches His Career at SpaceX Growing up in Westport, Connecticut, David Adipietro loved working on cars and wanted to pursue a career in automotive engineering, but â€¦\\n\\nFor Emma Phan, a sophomore chemistry major, the summer was a chance to dive into her research project related to ALS, a neurodegenerative disease. With help from recent graduate Beatriz â€¦\\n\\nMore than 30 undergraduates at Fordham College Rose Hill just completed a summer full of research, mentorship, and exploration. The second annual FCRH Summer Research Program, which had its final â€¦\\n\\nThis summer, Fordham received approval for a new major in biochemistry from the New York State Department of Education. The major aims to help students combine biology and chemistry work â€¦'},\n",
              "  {'page_name': 'Ccel',\n",
              "   'content': 'https://www.fordham.edu/ccel\\n\\n# Center for Community Engaged Learning\\n\\n#### Making an Impact - Here, There, and Everywhere\\n\\n17,463+\\nCommunity Members Engaged Annually\\n\\n5,890+\\nStudents in CCEL Courses and Leadership Programs Annually\\n\\n2,300+\\nCommunity Youth Engaged Through Post-Secondary Pathways\\n\\n102+\\nFordham Faculty Engaged Annually\\n\\nThe Fordham Center for Community Engaged Learning (CCEL) mission is to bridge the university with its neighboring communities and global partners through experiential learning, research, and civic engagement. It fosters reciprocal partnerships by connecting faculty, students, and staff with community organizations to integrate academic inquiry with community engagement, advancing justice and solidarity.\\n\\n### Spring 2026 Community Engaged Learning (CEL) Classes\\n\\n\\n## Visit Us\\n\\n**Rose Hill Campus **\\n\\n441 E. Fordham Road\\n\\nMcShane Campus Center, Suite 215\\n\\nBronx, NY 10458\\n\\n**Lincoln Center Campus**\\n\\n140 W. 62nd Street\\n\\nSuite 140 and 140A\\n\\nNew York, NY 10023'}])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Placeholder for your implementation\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "\n",
        "zip_path = Path(\"fordham-website.zip\")\n",
        "\n",
        "def clean_page_name(path_str: str) -> str:\n",
        "    \"\"\"\n",
        "    Turn 'about/index.html' or 'news-events.html' into a clean page name.\n",
        "    \"\"\"\n",
        "    # Take just the file name (last component)\n",
        "    fname = Path(path_str).name\n",
        "    # Strip extension\n",
        "    stem = Path(fname).stem\n",
        "    # Replace separators with spaces and title-case\n",
        "    clean = stem.replace(\"-\", \" \").replace(\"_\", \" \").strip().title()\n",
        "    return clean\n",
        "\n",
        "docs = []\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "    for info in zf.infolist():\n",
        "        # Skip directories\n",
        "        if info.is_dir():\n",
        "            continue\n",
        "        \n",
        "        # Read file bytes and decode as text\n",
        "        content_bytes = zf.read(info.filename)\n",
        "        try:\n",
        "            text = content_bytes.decode(\"utf-8\")\n",
        "        except UnicodeDecodeError:\n",
        "            # Fallback if some file has a different encoding\n",
        "            text = content_bytes.decode(\"latin-1\", errors=\"ignore\")\n",
        "        \n",
        "        page_name = clean_page_name(info.filename)\n",
        "        \n",
        "        docs.append({\n",
        "            \"page_name\": page_name,   # clean page name / fieldname\n",
        "            \"content\": text           # full file content as a string\n",
        "        })\n",
        "\n",
        "len(docs), docs[:3]  # quick sanity check\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 2. Chunk the Documents\n",
        "\n",
        "Some of the pages could be too long to embed as a single unit. Down the line, the pages may be too long to stuff into the LLM's prompt during the generation step. As such, most of the RAG systems will break down big documents into into smaller **chunks**.\n",
        "\n",
        "> ðŸ“š **TERM: Chunking**  \n",
        "> Splitting documents into smaller, self-contained pieces for embedding and retrieval. The goal is chunks that are small enough to be specific, but large enough to be meaningful.\n",
        "\n",
        "Your task: **write a function that splits each document into chunks.**\n",
        "\n",
        "Things to think about:\n",
        "- What's a reasonable chunk size? (Think about what fits in a prompt vs. what's too vague)\n",
        "- Should you split on sentences? Paragraphs? A fixed character/word count?\n",
        "- Should chunks overlap? What happens if an answer spans two chunks?\n",
        "- How do you keep track of which document each chunk came from? You may need that information down the line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 52,694 chunks from 9,560 documents\n",
            "Example chunk: {'page_name': 'Index', 'source_url': 'https://www.fordham.edu/', 'chunk_index': 0, 'content': '\\n## Doing Good That Becomes Greater As The Jesuit University of New York\\n\\nWeâ€™re located in New York Cityâ€”driven by our Jesuit values and tackling todayâ€™s most pressing issues at the center of the world stage.\\n\\n## We Are Leaders, Dreamers, Achievers, And Doers\\n\\nWith sound hearts, strong minds, and the wisdom to take charge, generations of Rams have found what they have needed to growâ€”the opportunities, connections, and support of this community.\\n\\n## From Winding Elms to Bustling City Blocks\\n\\nWith residential campuses in the Bronx and Manhattan, as well as campuses in Westchester and London, Fordham provides endless opportunities to start working toward your career and building the life you want.\\n\\n\\n## Weâ€™re Drawn to Where Weâ€™re Needed Most\\n\\nExplore how our values come to life: how Fordhamâ€™s students, faculty, and alumni contribute to society and make lives better.\\n\\n**Notice of Nondiscrimin'}\n"
          ]
        }
      ],
      "source": [
        "def chunk_documents(\n",
        "    docs: list[dict],\n",
        "    chunk_size: int = 900,\n",
        "    overlap: int = 0,\n",
        ") -> list[dict]:\n",
        "    \"\"\"\n",
        "    Split documents into fixed-size character chunks (default 900 chars).\n",
        "    Each chunk keeps metadata so you know which document it came from.\n",
        "\n",
        "    Args:\n",
        "        docs: List of dicts with \"page_name\" and \"content\" keys\n",
        "        chunk_size: Target number of characters per chunk\n",
        "        overlap: Number of characters to overlap between chunks (0 = no overlap)\n",
        "\n",
        "    Returns:\n",
        "        List of chunk dicts with page_name, source_url, chunk_index, content\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    chunk_id = 0\n",
        "\n",
        "    for doc in docs:\n",
        "        page_name = doc[\"page_name\"]\n",
        "        text = doc[\"content\"]\n",
        "\n",
        "        # First line is the URL; rest is content\n",
        "        lines = text.strip().split(\"\\n\")\n",
        "        source_url = lines[0].strip() if lines else \"\"\n",
        "        body = \"\\n\".join(lines[1:]) if len(lines) > 1 else \"\"\n",
        "\n",
        "        i = 0\n",
        "        chunk_index = 0\n",
        "\n",
        "        while i < len(body):\n",
        "            chunk_text = body[i: i + chunk_size]\n",
        "            chunk_dict = {\n",
        "                \"page_name\": page_name,\n",
        "                \"source_url\": source_url,\n",
        "                \"chunk_index\": chunk_index,\n",
        "                \"content\": chunk_text,\n",
        "            }\n",
        "            chunks.append(chunk_dict)\n",
        "            chunk_index += 1\n",
        "            if overlap > 0:\n",
        "                i += chunk_size - overlap\n",
        "            else:\n",
        "                i += chunk_size\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Run chunking on docs (from cell above)\n",
        "chunks = chunk_documents(docs)\n",
        "print(f\"Created {len(chunks):,} chunks from {len(docs):,} documents\")\n",
        "print(f\"Example chunk: {chunks[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 3. Embed the Chunks\n",
        "\n",
        "Now we need to turn each chunk into a vector so we can search over them. You've done this before in Lecture 4.\n",
        "\n",
        "Your task: **embed all chunks using an embedding model.**\n",
        "\n",
        "Tips:\n",
        "- You could use a local model, or API model. What are the tradeoffs?\n",
        "- This will take a while if you do it serially. You might want to use async/batch.\n",
        "- Once you've created your embeddings, you may want to save them to disk so you don't have to redo this step every time\n",
        "- You'll need to embed queries with the **same model** at search time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedded 500/52694 texts\n",
            "Embedded 1000/52694 texts\n",
            "Embedded 1500/52694 texts\n",
            "Embedded 2000/52694 texts\n",
            "Embedded 2500/52694 texts\n",
            "Embedded 3000/52694 texts\n",
            "Embedded 3500/52694 texts\n",
            "Embedded 4000/52694 texts\n",
            "Embedded 4500/52694 texts\n",
            "Embedded 5000/52694 texts\n",
            "Embedded 5500/52694 texts\n",
            "Embedded 6000/52694 texts\n",
            "Embedded 6500/52694 texts\n",
            "Embedded 7000/52694 texts\n",
            "Embedded 7500/52694 texts\n",
            "Embedded 8000/52694 texts\n",
            "Embedded 8500/52694 texts\n",
            "Embedded 9000/52694 texts\n",
            "Embedded 9500/52694 texts\n",
            "Embedded 10000/52694 texts\n",
            "Embedded 10500/52694 texts\n",
            "Embedded 11000/52694 texts\n",
            "Embedded 11500/52694 texts\n",
            "Embedded 12000/52694 texts\n",
            "Embedded 12500/52694 texts\n",
            "Embedded 13000/52694 texts\n",
            "Embedded 13500/52694 texts\n",
            "Embedded 14000/52694 texts\n",
            "Embedded 14500/52694 texts\n",
            "Embedded 15000/52694 texts\n",
            "Embedded 15500/52694 texts\n",
            "Embedded 16000/52694 texts\n",
            "Embedded 16500/52694 texts\n",
            "Embedded 17000/52694 texts\n",
            "Embedded 17500/52694 texts\n",
            "Embedded 18000/52694 texts\n",
            "Embedded 18500/52694 texts\n",
            "Embedded 19000/52694 texts\n",
            "Embedded 19500/52694 texts\n",
            "Embedded 20000/52694 texts\n",
            "Embedded 20500/52694 texts\n",
            "Embedded 21000/52694 texts\n",
            "Embedded 21500/52694 texts\n",
            "Embedded 22000/52694 texts\n",
            "Embedded 22500/52694 texts\n",
            "Embedded 23000/52694 texts\n",
            "Embedded 23500/52694 texts\n",
            "Embedded 24000/52694 texts\n",
            "Embedded 24500/52694 texts\n",
            "Embedded 25000/52694 texts\n",
            "Embedded 25500/52694 texts\n",
            "Embedded 26000/52694 texts\n",
            "Embedded 26500/52694 texts\n",
            "Embedded 27000/52694 texts\n",
            "Embedded 27500/52694 texts\n",
            "Embedded 28000/52694 texts\n",
            "Embedded 28500/52694 texts\n",
            "Embedded 29000/52694 texts\n",
            "Embedded 29500/52694 texts\n",
            "Embedded 30000/52694 texts\n",
            "Embedded 30500/52694 texts\n",
            "Embedded 31000/52694 texts\n",
            "Embedded 31500/52694 texts\n",
            "Embedded 32000/52694 texts\n",
            "Embedded 32500/52694 texts\n",
            "Embedded 33000/52694 texts\n",
            "Embedded 33500/52694 texts\n",
            "Embedded 34000/52694 texts\n",
            "Embedded 34500/52694 texts\n",
            "Embedded 35000/52694 texts\n",
            "Embedded 35500/52694 texts\n",
            "Embedded 36000/52694 texts\n",
            "Embedded 36500/52694 texts\n",
            "Embedded 37000/52694 texts\n",
            "Embedded 37500/52694 texts\n",
            "Embedded 38000/52694 texts\n",
            "Embedded 38500/52694 texts\n",
            "Embedded 39000/52694 texts\n",
            "Embedded 39500/52694 texts\n",
            "Embedded 40000/52694 texts\n",
            "Embedded 40500/52694 texts\n",
            "Embedded 41000/52694 texts\n",
            "Embedded 41500/52694 texts\n",
            "Embedded 42000/52694 texts\n",
            "Embedded 42500/52694 texts\n",
            "Embedded 43000/52694 texts\n",
            "Embedded 43500/52694 texts\n",
            "Embedded 44000/52694 texts\n",
            "Embedded 44500/52694 texts\n",
            "Embedded 45000/52694 texts\n",
            "Embedded 45500/52694 texts\n",
            "Embedded 46000/52694 texts\n",
            "Embedded 46500/52694 texts\n",
            "Embedded 47000/52694 texts\n",
            "Embedded 47500/52694 texts\n",
            "Embedded 48000/52694 texts\n",
            "Embedded 48500/52694 texts\n",
            "Embedded 49000/52694 texts\n",
            "Embedded 49500/52694 texts\n",
            "Embedded 50000/52694 texts\n",
            "Embedded 50500/52694 texts\n",
            "Embedded 51000/52694 texts\n",
            "Embedded 51500/52694 texts\n",
            "Embedded 52000/52694 texts\n",
            "Embedded 52500/52694 texts\n",
            "Embedded 52,694 chunks â†’ shape (52694, 1536)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure helpers is on path (run notebook from lectures/ or project root)\n",
        "_lectures = Path.cwd() / \"lectures\"\n",
        "if (_lectures / \"helpers.py\").exists():\n",
        "    sys.path.insert(0, str(_lectures))\n",
        "elif not (Path.cwd() / \"helpers.py\").exists():\n",
        "    sys.path.insert(0, \".\")\n",
        "\n",
        "from helpers import get_local_model, batch_embed_local, batch_embed_openai, batch_cosine_similarity\n",
        "from dotenv import load_dotenv\n",
        "import numpy as np\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# \"local\" = Hugging Face (default) | \"openai\" = OpenAI API\n",
        "EMBED_MODEL_TYPE = \"openai\"\n",
        "\n",
        "LOCAL_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "OPENAI_MODEL_NAME = \"text-embedding-3-small\"\n",
        "\n",
        "\n",
        "def get_embedding_model(model_type: str = EMBED_MODEL_TYPE):\n",
        "    \"\"\"Load the embedding model. Use same model at search time.\"\"\"\n",
        "    if model_type == \"local\":\n",
        "        return get_local_model(LOCAL_MODEL_NAME)\n",
        "    if model_type == \"openai\":\n",
        "        return None  # API handles calls; no model object\n",
        "    raise ValueError(f\"model_type must be 'local' or 'openai', got {model_type}\")\n",
        "\n",
        "\n",
        "def _truncate_for_openai(text: str, max_chars: int = 8000) -> str:\n",
        "    \"\"\"Simple safeguard: trim very long chunks before sending to OpenAI.\"\"\"\n",
        "    if len(text) <= max_chars:\n",
        "        return text\n",
        "    return text[:max_chars]\n",
        "\n",
        "\n",
        "def embed_texts(texts: list[str], model_type: str = EMBED_MODEL_TYPE, show_progress: bool = True) -> np.ndarray:\n",
        "    \"\"\"Embed a list of texts. Returns (n_texts, dim) numpy array.\"\"\"\n",
        "    if model_type == \"local\":\n",
        "        return batch_embed_local(texts, model_name=LOCAL_MODEL_NAME, show_progress=show_progress)\n",
        "    if model_type == \"openai\":\n",
        "        safe_texts = [_truncate_for_openai(t) for t in texts]\n",
        "        return batch_embed_openai(safe_texts, model=OPENAI_MODEL_NAME, batch_size=50, verbose=show_progress)\n",
        "    raise ValueError(f\"model_type must be 'local' or 'openai', got {model_type}\")\n",
        "\n",
        "\n",
        "# Set to True to only embed 100 chunks for a quick test\n",
        "TEST_MODE = False\n",
        "n_chunks = 100 if TEST_MODE else len(chunks)\n",
        "chunks_to_embed = chunks[:n_chunks]\n",
        "\n",
        "texts = [c[\"content\"] for c in chunks_to_embed]\n",
        "chunk_embeddings = embed_texts(texts, model_type=EMBED_MODEL_TYPE)\n",
        "print(f\"Embedded {len(chunks_to_embed):,} chunks â†’ shape {chunk_embeddings.shape}\")\n",
        "\n",
        "# Optional: save embeddings so you don't redo this step\n",
        "np.save(\"chunk_embeddings.npy\", chunk_embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 4. Retrieve\n",
        "\n",
        "Now build the **R** in RAG. Given a user's question, find the most relevant chunks.\n",
        "\n",
        "Your task: **write a retrieval function that takes a question and returns the most relevant chunks.**\n",
        "\n",
        "Tips:\n",
        "- You can use lexical or semantic search or both!\n",
        "- How many chunks should you retrieve? Too few and you might miss the answer; too many and you'll overwhelm the LLM (and pay more tokens)\n",
        "- Try a few test questions and eyeball whether the retrieved chunks are relevant\n",
        "- Try a few questions and see what comes back. For example:\n",
        "  - \"What programs does the Gabelli School of Business offer?\"\n",
        "  - \"How do I apply for financial aid?\"\n",
        "  - \"Where is Fordham's campus?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank 1 | score=0.7064 | page=School Of Professional And Continuing Studies Admissions And Aid Financial Aid And Scholarships\n",
            "Do I have to apply every year?**Yes. Financial aid is awarded on an annual basis.  **I don't think I will be eligible for aid. Should I still apply?**Absolutely! It is important to apply so that you c ...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 2 | score=0.6225 | page=Student Financial Services Undergraduate Financial Aid Current Students\n",
            " # Financial Aid Guidance for Current Students  Are you a current student looking to renew your financial aid or apply for new aid? Weâ€™ve got you covered.  Navigate this page:[Eligibility](#eligibilit ...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 3 | score=0.5989 | page=Student Financial Services Undergraduate Financial Aid Professional  Continuing Studies Students\n",
            " receive an award offer by email within four weeks of the completion of your financial aid application. A complete application includes responding to requests for any additional information from the O ...\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Dict, Any\n",
        "\n",
        "\n",
        "def retrieve_chunks(\n",
        "    question: str,\n",
        "    k: int = 5,\n",
        "    model_type: str = EMBED_MODEL_TYPE,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"Retrieve the top-k most relevant chunks for a question.\n",
        "\n",
        "    Args:\n",
        "        question: User question in plain text\n",
        "        k: Number of chunks to return\n",
        "        model_type: \"local\" or \"openai\" (must match how `chunk_embeddings` were created)\n",
        "\n",
        "    Returns:\n",
        "        List of chunk dicts augmented with rank and similarity score\n",
        "    \"\"\"\n",
        "    # 1. Embed the question using the same model type\n",
        "    if model_type == \"local\":\n",
        "        model = get_embedding_model(\"local\")\n",
        "        query_emb = model.encode(question, convert_to_numpy=True)\n",
        "    elif model_type == \"openai\":\n",
        "        q_text = _truncate_for_openai(question)\n",
        "        q_embs = batch_embed_openai([q_text], model=OPENAI_MODEL_NAME, batch_size=1, verbose=False)\n",
        "        query_emb = q_embs[0]\n",
        "    else:\n",
        "        raise ValueError(\"model_type must be 'local' or 'openai'\")\n",
        "\n",
        "    # 2. Compute cosine similarity against all chunk embeddings\n",
        "    scores = batch_cosine_similarity(query_emb, chunk_embeddings)\n",
        "\n",
        "    # 3. Get top-k indices\n",
        "    top_idx = np.argsort(-scores)[:k]\n",
        "\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    for rank, idx in enumerate(top_idx, start=1):\n",
        "        chunk = chunks_to_embed[idx]\n",
        "        results.append(\n",
        "            {\n",
        "                \"rank\": rank,\n",
        "                \"score\": float(scores[idx]),\n",
        "                **chunk,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Quick sanity check\n",
        "# sample_question = \"How do I apply for financial aid?\"\n",
        "# retrieved = retrieve_chunks(sample_question, k=3, model_type=EMBED_MODEL_TYPE)\n",
        "# for r in retrieved:\n",
        "#     print(f\"Rank {r['rank']} | score={r['score']:.4f} | page={r['page_name']}\")\n",
        "#     print(r[\"content\"][:200].replace(\"\\n\", \" \"), \"...\\n\")\n",
        "#     print(\"-\" * 80)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 5. Generate\n",
        "\n",
        "Now build the **G** in RAG. Take the retrieved chunks and pass them to an LLM along with the user's question.\n",
        "\n",
        "Your task: **write a function that takes a question and the retrieved chunks, builds a prompt, and calls an LLM to generate an answer.**\n",
        "\n",
        "Tips:\n",
        "- How should you structure the prompt? The LLM needs to know: (1) what is the context of the application, (2) what is the question, (3) what it should include in its answer\n",
        "- What should the LLM do if the context doesn't contain the answer?\n",
        "- Start with a cheap model; try a better one when you've figured out the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Full-time MBA (FTMBA) program at Fordham University is offered through the Gabelli School of Business and includes an onboarding component called Gabelli Launch, which is required for all incoming first-year students. This program starts in July and consists of both online and in-person elements, focusing on preparatory academic and experiential learning before the fall semester. It emphasizes skills in leadership development, teamwork, communication, technical skills with data analytics, problem-solving, and critical thinking.\n",
            "\n",
            "The FTMBA program is also STEM-designated, reflecting its integration of foundational business disciplines with technology, engineering, and science, preparing graduates for careers in a growing digital economy. The program fosters a close-knit community among students and utilizes advanced tools like Augmented Reality (AR) and Virtual Reality (VR) to enhance the classroom experience.\n"
          ]
        }
      ],
      "source": [
        "import litellm\n",
        "\n",
        "def generate_answer(\n",
        "    question: str,\n",
        "    retrieved_chunks: list[dict],\n",
        "    model: str = \"gpt-4o-mini\",\n",
        ") -> str:\n",
        "    \"\"\"Build a prompt from question + chunks, call LLM, return answer.\"\"\"\n",
        "    context_parts = []\n",
        "    for c in retrieved_chunks:\n",
        "        context_parts.append(f\"[Source: {c.get('page_name', 'Unknown')}]\\n{c['content']}\")\n",
        "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
        "\n",
        "    system = (\n",
        "        \"You are a helpful assistant that answers questions about Fordham University \"\n",
        "        \"using only the provided context. If the context does not contain the answer, \"\n",
        "        \"say so clearly. Do not make up information.\"\n",
        "    )\n",
        "    user = f\"Context:\\n\\n{context}\\n\\nQuestion: {question}\"\n",
        "\n",
        "    response = litellm.completion(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 6. Wire everything together\n",
        "\n",
        "Combine the previous steps into a simple function that takes in a question and returns an answer.\n",
        "\n",
        "Your task: **write a `rag(question)` function that retrieves relevant chunks and generates an answer.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Apostolos Filippas is an Assistant Professor at Fordham University's Gabelli School of Business, specifically focusing on Information, Technology, and Operations. He joined Fordham in 2019 and is also a research affiliate at the MIT Initiative on the Digital Economy. He is an economist who works on market design and the economics of technology, primarily in the context of online platforms. He teaches courses in Web Analytics and E-Commerce. His research addresses topics such as the design of reputation and pricing systems in online marketplaces, the economic and public policy implications of the \"sharing economy,\" and the design of social media platforms. Additionally, his work has been featured in various mainstream media outlets.\n"
          ]
        }
      ],
      "source": [
        "def rag(question: str, k: int = 5, model: str = \"gpt-4o-mini\") -> str:\n",
        "    \"\"\"User's question flows in â†’ retrieve â†’ generate_answer. Question in, answer out.\"\"\"\n",
        "    chunks = retrieve_chunks(question, k=k, model_type=EMBED_MODEL_TYPE)\n",
        "    return generate_answer(question, chunks, model=model)\n",
        "\n",
        "\n",
        "# Example: question is the input; it flows to retrieve, then to generate_answer\n",
        "user_question = input(\"Enter your question about Fordham University: \")\n",
        "answer = rag(user_question)\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 7. Evaluate, experiment and improve\n",
        "\n",
        "Your RAG system works â€” but there's always room to make it better. \n",
        "\n",
        "Your task: **evaluate, experiment, and improve your system**\n",
        "\n",
        "Tips:\n",
        "- How do you know that your system is working or that your changes are improving it?\n",
        "- Try different questions â€” where does it do well? Where does it struggle?\n",
        "- Adjust the number of retrieved chunks â€” what happens with more or fewer?\n",
        "- Try different chunking strategies â€” bigger chunks? Smaller? Overlap?\n",
        "- Try a different embedding model â€” does it change retrieval quality?\n",
        "- Improve the prompt â€” can you get better, more concise answers?\n",
        "- Add source attribution â€” can the system tell the user which pages the answer came from?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Placeholder for your implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 8. (Optional) Make it an app\n",
        "\n",
        "So far your RAG system lives inside a notebook. That's great for development â€” but nobody is going to use your Jupyter notebook to ask questions about Fordham. Let's turn it into a real web app.\n",
        "\n",
        "> ðŸ“š **TERM: Streamlit**  \n",
        "> A Python library that turns plain Python scripts into interactive web apps. You write Python â€” no HTML, CSS, or JavaScript â€” and Streamlit renders it as a web page with inputs, buttons, and formatted output. It's the fastest way to go from \"I have a function\" to \"I have a web app.\"\n",
        "\n",
        "Your task: **create a Streamlit app that lets a user type a question about Fordham and get an answer from your RAG system.**\n",
        "\n",
        "To get started:\n",
        "- Install it: `uv pip install streamlit` \n",
        "- A Streamlit app is just a `.py` file (not a notebook). Create something like `fordham_rag_app.py`\n",
        "- Run it: `streamlit run scripts/fordham_rag_app.py` â€” this opens a browser tab with your app\n",
        "\n",
        "Tips:\n",
        "- Check out the [Streamlit docs](https://docs.streamlit.io/) â€” the \"Get started\" tutorial is very short\n",
        "- Your best bet is to vibecode your way to this. You'll be surprised how fast you can get it up and running"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "## What You Built\n",
        "\n",
        "| Step | What You Did | What It Does |\n",
        "|------|-------------|-------------|\n",
        "| **Load** | Read 9,500+ Fordham web pages | Get raw content |\n",
        "| **Chunk** | Split pages into smaller pieces | Make content searchable and promptable |\n",
        "| **Embed** | Turn chunks into vectors | Enable semantic search |\n",
        "| **Retrieve** | Find relevant chunks for a question | The **R** in RAG |\n",
        "| **Generate** | Ask an LLM to answer using the chunks | The **G** in RAG |\n",
        "| **RAG** | Wire it all together | Question in, answer out |\n",
        "\n",
        "## The Big Picture\n",
        "\n",
        "RAG is one of the most common patterns in AI engineering today. What you built here is the same core architecture behind tools like ChatGPT with search, Perplexity, enterprise Q&A bots, and more. The details get more sophisticated (vector databases, reranking, query rewriting, evaluation) but the pattern is the same:\n",
        "\n",
        "**Find relevant stuff â†’ give it to an LLM â†’ get an answer.**\n",
        "\n",
        "You can just build things."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (ai-engineering-fordham)",
      "language": "python",
      "name": "ai-engineering-fordham"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
