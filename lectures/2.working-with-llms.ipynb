{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
    "\n",
    "### ðŸ“˜ **Class**: AI Engineering\n",
    "\n",
    "### ðŸ“‹ **Topic**: Working with Large Language Models (LLMs) via API\n",
    "\n",
    "ðŸš« **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Today we'll learn how to work with Large Language Models (LLMs) through their APIs. By the end of this lecture, you'll be able to:\n",
    "\n",
    "1. **Use Provider SDKs** - Call OpenAI, Anthropic, and Google directly\n",
    "2. **Use LiteLLM** - One unified interface for all providers\n",
    "3. **Handle Failures** - Implement retries with exponential backoff\n",
    "4. **Understand Pydantic** - Python's data validation library\n",
    "5. **Get Structured Outputs** - Guaranteed JSON responses from LLMs\n",
    "6. **Use Async** - Make concurrent API calls for speed\n",
    "7. **Use Instructor** - Alternative approach to structured outputs\n",
    "8. **Generate Images** - Create images with Google Imagen\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Setup: API Keys and Environment\n",
    "\n",
    "Before we can call any LLM, we need API keys. These are like passwords that identify you to the service.\n",
    "\n",
    "## 1.1 Getting API Keys\n",
    "\n",
    "| Provider | Where to Get Key | Cost |\n",
    "|----------|-----------------|------|\n",
    "| **OpenAI** | [platform.openai.com](https://platform.openai.com) | Pay-as-you-go |\n",
    "| **Anthropic** | [console.anthropic.com](https://console.anthropic.com) | Pay-as-you-go |\n",
    "| **Google** | [aistudio.google.com](https://aistudio.google.com) | Free tier available |\n",
    "\n",
    "## 1.2 Storing Keys Securely\n",
    "\n",
    "Never put API keys in your code! Instead, use a `.env` file (in your project root)\n",
    "```python\n",
    "OPENAI_API_KEY=sk-your-key-here\n",
    "ANTHROPIC_API_KEY=sk-ant-your-key-here\n",
    "GOOGLE_API_KEY=your-google-key-here\n",
    "```\n",
    "\n",
    "Make sure `.env` is in your `.gitignore` so you don't accidentally commit your keys!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI key loaded: True\n",
      "Anthropic key loaded: True\n",
      "Google key loaded: True\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Suppress noisy warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify keys are loaded (don't print actual keys!)\n",
    "print(\"OpenAI key loaded:\", \"OPENAI_API_KEY\" in os.environ)\n",
    "print(\"Anthropic key loaded:\", \"ANTHROPIC_API_KEY\" in os.environ)\n",
    "print(\"Google key loaded:\", \"GOOGLE_API_KEY\" in os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Provider SDKs: The Native Way\n",
    "\n",
    "Each LLM provider has their own Python SDK. Let's see how each one works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 OpenAI SDK\n",
    "\n",
    "OpenAI's SDK is the most widely used. It powers the GPT-4.1 series and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Response:\n",
      "Python is a high-level, interpreted, general-purpose programming language created by Guido van Rossum that emphasizes readability, simplicity, and rapid development.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Create a client (automatically uses OPENAI_API_KEY from environment)\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Make a chat completion request\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is Python in exactly one sentence?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract the response\n",
    "print(\"OpenAI Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Response Structure\n",
    "\n",
    "The response is a `ChatCompletion` object. Key parts:\n",
    "\n",
    "```python\n",
    "response.choices[0].message.content  # The actual text response\n",
    "response.choices[0].finish_reason    # Why generation stopped (\"stop\", \"length\", etc.)\n",
    "response.usage.total_tokens          # Tokens used (for billing)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Anthropic SDK\n",
    "\n",
    "Anthropic makes Claude models. Their API is slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic Response:\n",
      "Python is a high-level, interpreted programming language known for its simple, readable syntax that makes it accessible to beginners while remaining powerful enough for professional software development.\n"
     ]
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "# Create a client\n",
    "anthropic_client = Anthropic()\n",
    "\n",
    "# Make a message request (note: different method name!)\n",
    "response = anthropic_client.messages.create(\n",
    "    model=\"claude-haiku-4-5-20251001\",\n",
    "    max_tokens=1024,  # Required for Anthropic!\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is Python in exactly one sentence?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract the response (different structure!)\n",
    "print(\"Anthropic Response:\")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic Response Structure\n",
    "\n",
    "Different from OpenAI:\n",
    "\n",
    "```python\n",
    "response.content[0].text    # The actual text (not .message.content!)\n",
    "response.stop_reason        # Why it stopped\n",
    "response.usage.input_tokens # Tokens used\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Google Gemini SDK\n",
    "\n",
    "Google's SDK is different again. They use the `google-genai` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "# Create a client\n",
    "google_client = genai.Client(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Generate content (yet another API style!)\n",
    "response = google_client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is Python in exactly one sentence?\"\n",
    ")\n",
    "\n",
    "# Extract the response\n",
    "print(\"Google Response:\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 The Problem: Three APIs, Three Formats\n",
    "\n",
    "Notice how each provider has:\n",
    "- Different client initialization\n",
    "- Different method names (`chat.completions.create` vs `messages.create` vs `generate_content`)\n",
    "- Different response structures\n",
    "- Different parameter names\n",
    "\n",
    "This makes it hard to:\n",
    "- Switch providers\n",
    "- Compare models\n",
    "- Write reusable code\n",
    "\n",
    "**Solution: LiteLLM** - One API to rule them all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. LiteLLM: Unified Interface\n",
    "\n",
    "LiteLLM provides a single, consistent API that works with 100+ LLM providers.\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **One API** | Same code for any provider |\n",
    "| **Easy Switching** | Change one line to switch models |\n",
    "| **Fallbacks** | Automatically try another provider if one fails |\n",
    "| **Cost Tracking** | Built-in usage monitoring |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "# Same function works with ANY provider!\n",
    "def ask_llm(prompt: str, model: str = \"gpt-5-mini\") -> str:\n",
    "    \"\"\"Ask any LLM a question using LiteLLM.\"\"\"\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-mini: Bonjour\n"
     ]
    }
   ],
   "source": [
    "# OpenAI\n",
    "print(\"GPT-5-mini:\", ask_llm(\"Say 'hello' in French\", model=\"gpt-5-mini\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "litellm.NotFoundError: AnthropicException - {\"type\":\"error\",\"error\":{\"type\":\"not_found_error\",\"message\":\"model: claude-3-5-haiku-20241022\"},\"request_id\":\"req_011CXWeCU7FAbhrQiCWeN3Yf\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafay\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\litellm\\llms\\anthropic\\chat\\handler.py:446\u001b[39m, in \u001b[36mAnthropicChatCompletion.completion\u001b[39m\u001b[34m(self, model, messages, api_base, custom_llm_provider, custom_prompt_dict, model_response, print_verbose, encoding, api_key, logging_obj, optional_params, timeout, litellm_params, acompletion, logger_fn, headers, client)\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafay\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:979\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    978\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m\"\u001b[39m, e.response.status_code)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafay\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:961\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    960\u001b[39m response = \u001b[38;5;28mself\u001b[39m.client.send(req, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafay\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\httpx\\_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '404 Not Found' for url 'https://api.anthropic.com/v1/messages'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAnthropicError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafay\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\litellm\\main.py:2665\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   2661\u001b[39m     verbose_logger.debug(\n\u001b[32m   2662\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLITELLM_ANTHROPIC_DISABLE_URL_SUFFIX is set, skipping /v1/messages suffix\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2663\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2665\u001b[39m response = \u001b[43manthropic_chat_completions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2669\u001b[39m \u001b[43m    \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2673\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_get_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# for calculating input/output tokens\u001b[39;49;00m\n\u001b[32m   2677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2683\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optional_params.get(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m acompletion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   2685\u001b[39m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafay\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\litellm\\llms\\anthropic\\chat\\handler.py:461\u001b[39m, in \u001b[36mAnthropicChatCompletion.completion\u001b[39m\u001b[34m(self, model, messages, api_base, custom_llm_provider, custom_prompt_dict, model_response, print_verbose, encoding, api_key, logging_obj, optional_params, timeout, litellm_params, acompletion, logger_fn, headers, client)\u001b[39m\n\u001b[32m    460\u001b[39m                 error_text = \u001b[38;5;28mgetattr\u001b[39m(error_response, \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, error_text)\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m AnthropicError(\n\u001b[32m    462\u001b[39m                 message=error_text,\n\u001b[32m    463\u001b[39m                 status_code=status_code,\n\u001b[32m    464\u001b[39m                 headers=error_headers,\n\u001b[32m    465\u001b[39m             )\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m config.transform_response(\n\u001b[32m    468\u001b[39m     model=model,\n\u001b[32m    469\u001b[39m     raw_response=response,\n\u001b[32m   (...)\u001b[39m\u001b[32m    478\u001b[39m     json_mode=json_mode,\n\u001b[32m    479\u001b[39m )\n",
      "\u001b[31mAnthropicError\u001b[39m: {\"type\":\"error\",\"error\":{\"type\":\"not_found_error\",\"message\":\"model: claude-3-5-haiku-20241022\"},\"request_id\":\"req_011CXWeCU7FAbhrQiCWeN3Yf\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Anthropic (same code, different model!)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mClaude Haiku:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mask_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSay \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhello\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m in French\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclaude-3-5-haiku-20241022\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mask_llm\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mask_llm\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mgpt-5-mini\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m      5\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Ask any LLM a question using LiteLLM.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     response = \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafay\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\litellm\\utils.py:1739\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1736\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1737\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1738\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafay\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\litellm\\utils.py:1560\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1558\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1559\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1561\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1563\u001b[39m     kwargs=kwargs,\n\u001b[32m   1564\u001b[39m     call_type=call_type,\n\u001b[32m   1565\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafay\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\litellm\\main.py:4205\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   4202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   4203\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   4204\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4208\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4211\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafay\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2356\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2355\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2356\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2358\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafay\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:665\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m404\u001b[39m:\n\u001b[32m    664\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotFoundError(\n\u001b[32m    666\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnthropicException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    667\u001b[39m         model=model,\n\u001b[32m    668\u001b[39m         llm_provider=\u001b[33m\"\u001b[39m\u001b[33manthropic\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    669\u001b[39m     )\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m408\u001b[39m:\n\u001b[32m    671\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mNotFoundError\u001b[39m: litellm.NotFoundError: AnthropicException - {\"type\":\"error\",\"error\":{\"type\":\"not_found_error\",\"message\":\"model: claude-3-5-haiku-20241022\"},\"request_id\":\"req_011CXWeCU7FAbhrQiCWeN3Yf\"}"
     ]
    }
   ],
   "source": [
    "# Anthropic (same code, different model!)\n",
    "print(\"Claude Haiku:\", ask_llm(\"Say 'hello' in French\", model=\"claude-3-5-haiku-20241022\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Flash: Bonjour!\n"
     ]
    }
   ],
   "source": [
    "# Google (same code, just add \"gemini/\" prefix)\n",
    "print(\"Gemini Flash:\", ask_llm(\"Say 'hello' in French\", model=\"gemini/gemini-2.5-flash\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Name Reference\n",
    "\n",
    "| Provider | Model Name | Notes |\n",
    "|----------|-----------|-------|\n",
    "| **OpenAI** | `gpt-5.2` | Most capable (latest) |\n",
    "| | `gpt-5.2-pro` | More compute, better answers |\n",
    "| | `gpt-5.1` | Great for coding/agentic tasks |\n",
    "| | `gpt-5-mini` | Fast and cheap |\n",
    "| **Anthropic** | `claude-opus-4-5-20251101` | Most capable |\n",
    "| | `claude-sonnet-4-5-20250929` | Best balance |\n",
    "| | `claude-3-5-haiku-20241022` | Fast and cheap |\n",
    "| **Google** | `gemini/gemini-2.5-pro` | Most capable |\n",
    "| | `gemini/gemini-2.5-flash` | Fast and cheap |\n",
    "\n",
    "**Note**: We use `gpt-5-mini` in examples because it's cost-effective for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Messages and Conversation History\n",
    "\n",
    "LLMs understand different \"roles\" in a conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        # System message: sets the behavior/personality\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful pirate. Always respond like a pirate.\"},\n",
    "        # User message: the actual question\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather like?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Making API Calls Robust: Retries\n",
    "\n",
    "API calls can fail for many reasons:\n",
    "- **Rate limiting** (HTTP 429) - too many requests\n",
    "- **Server errors** (HTTP 500) - provider issues\n",
    "- **Timeouts** - network issues\n",
    "\n",
    "**Solution:** Retry with exponential backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Manual Retry Pattern\n",
    "\n",
    "Here's how to implement retries from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def call_with_retries(prompt: str, max_retries: int = 5, base_delay: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Call LLM with exponential backoff retry.\n",
    "    \n",
    "    Wait times: 1s -> 2s -> 4s -> 8s -> 16s (plus random jitter)\n",
    "    \"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = litellm.completion(\n",
    "                model=\"gpt-5-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries:\n",
    "                raise  # Give up after max retries\n",
    "            \n",
    "            # Exponential backoff with jitter\n",
    "            delay = base_delay * (2 ** (attempt - 1)) + random.random()\n",
    "            print(f\"Attempt {attempt} failed: {e}. Retrying in {delay:.1f}s...\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "# Test it\n",
    "result = call_with_retries(\"Say 'hello'\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Using Tenacity Library\n",
    "\n",
    "Writing retry logic is tedious. The `tenacity` library makes it elegant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),           # Max 5 attempts\n",
    "    wait=wait_exponential(multiplier=1, min=1, max=60),  # Exponential backoff\n",
    "    reraise=True                          # Re-raise the exception if all retries fail\n",
    ")\n",
    "def robust_llm_call(prompt: str, model: str = \"gpt-5-mini\") -> str:\n",
    "    \"\"\"Call LLM with automatic retries.\"\"\"\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# The decorator handles all retry logic!\n",
    "result = robust_llm_call(\"What is 2 + 2?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Built-in Client Retries\n",
    "\n",
    "The OpenAI client has built-in retry support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Create client with automatic retries\n",
    "client = OpenAI(\n",
    "    max_retries=5,  # Automatically retry up to 5 times\n",
    "    timeout=30.0    # Timeout after 30 seconds\n",
    ")\n",
    "\n",
    "# Now all calls through this client will automatically retry!\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. What is Pydantic?\n",
    "\n",
    "**Pydantic** is Python's most popular data validation library. It lets you:\n",
    "\n",
    "- Define data structures with types\n",
    "- Automatically validate data\n",
    "- Get helpful error messages\n",
    "- Serialize to/from JSON\n",
    "\n",
    "It's used everywhere in modern Python: FastAPI, LangChain, Django Ninja, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Basic Pydantic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define a data structure\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: int = Field(ge=0, le=150)  # Must be 0-150\n",
    "    email: str | None = None        # Optional field\n",
    "\n",
    "# Create an instance - Pydantic validates automatically!\n",
    "person = Person(name=\"Alice\", age=30, email=\"alice@example.com\")\n",
    "print(person)\n",
    "print(f\"Name: {person.name}, Age: {person.age}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic catches invalid data\n",
    "try:\n",
    "    invalid_person = Person(name=\"Bob\", age=200)  # Age > 150!\n",
    "except Exception as e:\n",
    "    print(f\"Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic auto-converts types when possible\n",
    "person = Person(name=\"Charlie\", age=\"25\")  # String \"25\" -> int 25\n",
    "print(f\"Age is {person.age}, type: {type(person.age)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Serialization (to/from JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to JSON\n",
    "person = Person(name=\"Diana\", age=28)\n",
    "json_str = person.model_dump_json(indent=2)\n",
    "print(\"As JSON:\")\n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse from JSON\n",
    "json_data = '{\"name\": \"Eve\", \"age\": 35, \"email\": \"eve@example.com\"}'\n",
    "person = Person.model_validate_json(json_data)\n",
    "print(f\"Parsed: {person.name}, {person.age}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Why Pydantic Matters for LLMs\n",
    "\n",
    "LLMs return free-form text. Pydantic lets us:\n",
    "\n",
    "1. **Define what we want** - Create a schema/model\n",
    "2. **Get structured data** - LLM returns JSON matching our schema\n",
    "3. **Validate automatically** - Pydantic checks the data is correct\n",
    "4. **Use easily** - Access fields with dot notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Structured Outputs with Pydantic\n",
    "\n",
    "The **problem**: LLMs return free-form text that's hard to parse.\n",
    "\n",
    "Ask \"extract info from this review\" and you might get:\n",
    "- \"The sentiment is positive and the rating is 4.5\"\n",
    "- \"Rating: 4.5/5, Sentiment: positive\"\n",
    "- \"Positive review! 4.5 stars.\"\n",
    "\n",
    "**Solution**: Use `response_format` to get guaranteed JSON structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "# Define the structure we want\n",
    "class MovieReview(BaseModel):\n",
    "    \"\"\"Structured data extracted from a movie review.\"\"\"\n",
    "    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = Field(\n",
    "        description=\"The overall sentiment of the review\"\n",
    "    )\n",
    "    rating: float = Field(\n",
    "        description=\"Numeric rating from 1.0 to 5.0\",\n",
    "        ge=1.0,\n",
    "        le=5.0\n",
    "    )\n",
    "    key_points: list[str] = Field(\n",
    "        description=\"Main points mentioned in the review (1-5 items)\",\n",
    "        min_length=1,\n",
    "        max_length=5\n",
    "    )\n",
    "    reviewer_name: str | None = Field(\n",
    "        default=None,\n",
    "        description=\"Name of the reviewer if mentioned\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample review to analyze\n",
    "review_text = \"\"\"\n",
    "This movie was absolutely fantastic! The cinematography was stunning, \n",
    "and the acting performances were top-notch. I'd give it 4.5 stars. \n",
    "The plot kept me engaged from start to finish. Highly recommend!\n",
    "- Sarah Johnson\n",
    "\"\"\"\n",
    "\n",
    "# Use LiteLLM with response_format\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Extract structured information from movie reviews.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Extract information from this review:\\n\\n{review_text}\"\n",
    "        }\n",
    "    ],\n",
    "    response_format=MovieReview  # Tell the LLM to return this structure!\n",
    ")\n",
    "\n",
    "# Parse the JSON response into our Pydantic model\n",
    "review_data = MovieReview.model_validate_json(response.choices[0].message.content)\n",
    "\n",
    "# Now we have clean, typed data!\n",
    "print(f\"Sentiment: {review_data.sentiment}\")\n",
    "print(f\"Rating: {review_data.rating}\")\n",
    "print(f\"Key Points: {review_data.key_points}\")\n",
    "print(f\"Reviewer: {review_data.reviewer_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Nested Models\n",
    "\n",
    "You can create complex structures with nested Pydantic models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(BaseModel):\n",
    "    \"\"\"Information about an actor.\"\"\"\n",
    "    name: str\n",
    "    role: str\n",
    "\n",
    "class MovieInfo(BaseModel):\n",
    "    \"\"\"Comprehensive movie information.\"\"\"\n",
    "    title: str\n",
    "    year: int = Field(ge=1900, le=2030)\n",
    "    genre: list[str]\n",
    "    director: str\n",
    "    actors: list[Actor]  # Nested model!\n",
    "    plot_summary: str = Field(max_length=500)\n",
    "\n",
    "# Use it\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me information about the movie Inception\"}\n",
    "    ],\n",
    "    response_format=MovieInfo\n",
    ")\n",
    "\n",
    "movie = MovieInfo.model_validate_json(response.choices[0].message.content)\n",
    "print(f\"Title: {movie.title} ({movie.year})\")\n",
    "print(f\"Director: {movie.director}\")\n",
    "print(f\"Genres: {', '.join(movie.genre)}\")\n",
    "print(f\"\\nActors:\")\n",
    "for actor in movie.actors:\n",
    "    print(f\"  - {actor.name} as {actor.role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Async Programming\n",
    "\n",
    "**Problem**: If you need to make 100 LLM calls, doing them one-by-one is slow.\n",
    "\n",
    "**Solution**: Make them concurrently with async programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 What is Async?\n",
    "\n",
    "Think of it like ordering at a restaurant:\n",
    "\n",
    "- **Synchronous**: Order one dish, wait for it, eat it, then order the next\n",
    "- **Asynchronous**: Order all dishes at once, they arrive as they're ready\n",
    "\n",
    "Key Python concepts:\n",
    "\n",
    "| Keyword | Meaning |\n",
    "|---------|---------|\n",
    "| `async def` | Defines an async function |\n",
    "| `await` | Pause here until the result is ready |\n",
    "| `asyncio.gather()` | Run multiple async tasks concurrently |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Sequential vs Concurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# A list of prompts to process\n",
    "prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is the capital of Germany?\",\n",
    "    \"What is the capital of Italy?\",\n",
    "    \"What is the capital of Spain?\",\n",
    "    \"What is the capital of Portugal?\",\n",
    "    \"What is the capital of Greece?\",\n",
    "    \"What is the capital of Turkey?\",\n",
    "    \"What is the capital of Bulgaria?\",\n",
    "    \"What is the capital of Romania?\",\n",
    "    \"What is the capital of Hungary?\",\n",
    "    \"What is the capital of Poland?\",\n",
    "    \"What is the capital of Czech Republic?\",\n",
    "]\n",
    "\n",
    "# Sequential: one at a time\n",
    "start = time.time()\n",
    "sequential_results = []\n",
    "for prompt in prompts:\n",
    "    response = litellm.completion(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    sequential_results.append(response.choices[0].message.content)\n",
    "sequential_time = time.time() - start\n",
    "\n",
    "print(f\"Sequential: {sequential_time:.2f} seconds\")\n",
    "for prompt, result in zip(prompts, sequential_results):\n",
    "    print(f\"  {prompt} -> {result[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Async function to call LLM\n",
    "async def async_ask(prompt: str) -> str:\n",
    "    \"\"\"Make an async LLM call.\"\"\"\n",
    "    response = await litellm.acompletion(  # Note: acompletion, not completion!\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Async function to process all prompts concurrently\n",
    "async def process_all(prompts: list[str]) -> list[str]:\n",
    "    \"\"\"Process all prompts concurrently.\"\"\"\n",
    "    tasks = [async_ask(prompt) for prompt in prompts]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "# Run concurrently\n",
    "start = time.time()\n",
    "concurrent_results = await process_all(prompts)\n",
    "concurrent_time = time.time() - start\n",
    "\n",
    "print(f\"Concurrent: {concurrent_time:.2f} seconds\")\n",
    "print(f\"Speedup: {sequential_time / concurrent_time:.1f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 When to Use Async\n",
    "\n",
    "Use async when:\n",
    "- Processing many items (batch analysis)\n",
    "- Making independent API calls\n",
    "- Building web applications (FastAPI uses async)\n",
    "\n",
    "Don't bother for:\n",
    "- Single API calls\n",
    "- Sequential workflows where each step depends on the previous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. Instructor Library\n",
    "\n",
    "**Instructor** is another approach to structured outputs. It:\n",
    "\n",
    "- Patches existing clients (OpenAI, Anthropic, etc.)\n",
    "- Returns Pydantic objects directly (not JSON strings)\n",
    "- Has built-in retry and validation\n",
    "- Works with multiple providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Patch the OpenAI client with Instructor\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "# Define what we want\n",
    "class UserInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    occupation: str\n",
    "\n",
    "# Use it - returns a Pydantic object directly!\n",
    "user = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Generate a random person's info\"}\n",
    "    ],\n",
    "    response_model=UserInfo  # Instructor uses response_model, not response_format\n",
    ")\n",
    "\n",
    "# user is already a UserInfo object!\n",
    "print(f\"Name: {user.name}\")\n",
    "print(f\"Age: {user.age}\")\n",
    "print(f\"Occupation: {user.occupation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Instructor with Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "# Patch Anthropic client\n",
    "anthropic_instructor = instructor.from_anthropic(Anthropic())\n",
    "\n",
    "# Same interface!\n",
    "user = anthropic_instructor.messages.create(\n",
    "    model=\"claude-3-5-haiku-20241022\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Generate a random person's info\"}\n",
    "    ],\n",
    "    response_model=UserInfo\n",
    ")\n",
    "\n",
    "print(f\"From Claude: {user.name}, {user.age}, {user.occupation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Instructor vs LiteLLM: When to Use Each\n",
    "\n",
    "| Feature | LiteLLM | Instructor |\n",
    "|---------|---------|------------|\n",
    "| **Multi-provider** | Yes (100+ providers) | Yes (OpenAI, Anthropic, etc.) |\n",
    "| **Structured output** | Returns JSON string | Returns Pydantic object |\n",
    "| **Retry built-in** | No (use tenacity) | Yes |\n",
    "| **Validation retry** | No | Yes (retries if validation fails) |\n",
    "| **Learning curve** | Lower | Slightly higher |\n",
    "\n",
    "**Use LiteLLM when**: You want unified access to many providers\n",
    "\n",
    "**Use Instructor when**: You want the cleanest structured output experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 9. Image Generation with Gemini (Nano Banana)\n",
    "\n",
    "Gemini models can generate images natively - no separate Imagen model needed! This feature is called \"Nano Banana\" internally at Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from pathlib import Path\n",
    "\n",
    "# Create client\n",
    "google_client = genai.Client(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Generate an image using Nano Banana (Gemini's native image generation)\n",
    "response = google_client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-image\",  # Nano Banana model\n",
    "    contents=[\"A cute robot learning to code, digital art style\"],\n",
    ")\n",
    "\n",
    "# Find and save the image from the response\n",
    "output_path = Path(\"temp/robot_coding.png\")\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "for part in response.parts:\n",
    "    if part.inline_data is not None:\n",
    "        image = part.as_image()\n",
    "        image.save(output_path)\n",
    "        print(f\"Image saved to {output_path}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image in Jupyter\n",
    "from IPython.display import Image, display\n",
    "\n",
    "if output_path.exists():\n",
    "    display(Image(filename=str(output_path), width=400))\n",
    "else:\n",
    "    print(\"Run the previous cell first to generate the image!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Nano Banana Pro (Gemini 3)\n",
    "\n",
    "For complex prompts that need reasoning, use **Nano Banana Pro**. It \"thinks\" before generating, making it great for:\n",
    "- Infographics with text\n",
    "- Complex compositions\n",
    "- Multi-element scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nano Banana Pro - uses \"thinking\" for complex prompts\n",
    "from google.genai import types\n",
    "\n",
    "response = google_client.models.generate_content(\n",
    "    model=\"gemini-3-pro-image-preview\",  # Nano Banana Pro\n",
    "    contents=[\"Create a detailed infographic showing how neural networks learn, \"\n",
    "              \"with clear labels, arrows showing data flow, and a modern tech aesthetic\"],\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_modalities=['TEXT', 'IMAGE'],\n",
    "        thinking_config=types.ThinkingConfig(\n",
    "            include_thoughts=True  # Enable \"thinking\" mode\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save the pro image\n",
    "pro_output_path = Path(\"temp/neural_network_infographic.png\")\n",
    "\n",
    "for part in response.parts:\n",
    "    if part.inline_data is not None:\n",
    "        image = part.as_image()\n",
    "        image.save(pro_output_path)\n",
    "        print(f\"Pro image saved to {pro_output_path}\")\n",
    "        break\n",
    "    elif part.text is not None:\n",
    "        print(f\"Model's thoughts: {part.text[:200]}...\")  # Show reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Pro image\n",
    "if pro_output_path.exists():\n",
    "    display(Image(filename=str(pro_output_path), width=500))\n",
    "else:\n",
    "    print(\"Run the previous cell first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Tips for Image Generation\n",
    "\n",
    "**Nano Banana models**:\n",
    "| Model | ID | Best For |\n",
    "|-------|-----|----------|\n",
    "| **Nano Banana** | `gemini-2.5-flash-image` | Fast, high-volume tasks |\n",
    "| **Nano Banana Pro** | `gemini-3-pro-image-preview` | Pro quality, complex prompts |\n",
    "\n",
    "**Good prompts**:\n",
    "- Be specific: \"A golden retriever puppy playing in autumn leaves, photograph\"\n",
    "- Include style: \"digital art\", \"oil painting\", \"photograph\", \"3D render\"\n",
    "- Describe lighting: \"sunset lighting\", \"studio lighting\"\n",
    "\n",
    "**With aspect ratio**:\n",
    "```python\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-image\",\n",
    "    contents=[\"your prompt\"],\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_modalities=['TEXT', 'IMAGE'],\n",
    "        image_config=types.ImageConfig(aspect_ratio=\"16:9\")\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "**Edit existing images**:\n",
    "```python\n",
    "from PIL import Image\n",
    "img = Image.open(\"photo.png\")\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-image\",\n",
    "    contents=[\"Add a wizard hat\", img],  # Pass image + edit instruction\n",
    ")\n",
    "```\n",
    "\n",
    "**Note**: All images include an invisible SynthID watermark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 10. Summary\n",
    "\n",
    "Today we covered a lot! Here's a quick reference:\n",
    "\n",
    "| Topic | Key Takeaway |\n",
    "|-------|-------------|\n",
    "| **Provider SDKs** | Each provider has different APIs (OpenAI, Anthropic, Google) |\n",
    "| **LiteLLM** | Unified interface for all providers - use this! |\n",
    "| **Retries** | Use `tenacity` or built-in client retries for robustness |\n",
    "| **Pydantic** | Data validation library for defining schemas |\n",
    "| **Structured Outputs** | Use `response_format` for guaranteed JSON |\n",
    "| **Async** | Use `acompletion` + `asyncio.gather` for concurrent calls |\n",
    "| **Instructor** | Alternative for structured outputs with validation retries |\n",
    "| **Nano Banana** | Gemini's native image generation (`gemini-2.5-flash-image`) |\n",
    "\n",
    "## Quick Code Reference\n",
    "\n",
    "```python\n",
    "# Basic LLM call with LiteLLM\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "\n",
    "# Structured output\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[...],\n",
    "    response_format=MyPydanticModel\n",
    ")\n",
    "data = MyPydanticModel.model_validate_json(response.choices[0].message.content)\n",
    "\n",
    "# Async call\n",
    "response = await litellm.acompletion(model=\"gpt-5-mini\", messages=[...])\n",
    "\n",
    "# Image generation (Nano Banana)\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-image\",\n",
    "    contents=[\"your prompt\"]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [LiteLLM Documentation](https://docs.litellm.ai)\n",
    "- [Pydantic Documentation](https://docs.pydantic.dev)\n",
    "- [Instructor Documentation](https://python.useinstructor.com)\n",
    "- [OpenAI API Reference](https://platform.openai.com/docs)\n",
    "- [Anthropic API Reference](https://docs.anthropic.com)\n",
    "- [Google Gemini API](https://ai.google.dev/gemini-api/docs)\n",
    "- [Tenacity Documentation](https://tenacity.readthedocs.io)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai-engineering-fordham)",
   "language": "python",
   "name": "ai-engineering-fordham"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
