{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
    "\n",
    "### ðŸ“˜ **Class**: AI Engineering\n",
    "\n",
    "### ðŸ“‹ **Topic**: Evaluations\n",
    "\n",
    "ðŸš« **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "In our previous lecture, we built a full RAG pipeline. That's great â€” you can just build things! But how do you know if they're any good? How do you know if one approach is better than another? How do you improve your system.. systematically?\n",
    "\n",
    "The answer to that is **evaluations** â€” the systematic measurement of how well your AI system performs. Without evals, you're flying blind. All big AI labs and companies have extensive evaluation harnesses for their systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Hugging Face\n",
    "\n",
    "> ðŸ“š **TERM: Hugging Face**  \n",
    "> An open platform for sharing ML models, datasets, and applications. Think of it as GitHub for AI â€” anyone can upload models or datasets, and you can download and use them with a few lines of code.\n",
    "\n",
    "Hugging Face hosts:\n",
    "- **Models** â€” pre-trained models for text, images, audio, etc.\n",
    "- **Datasets** â€” curated datasets for training and evaluation\n",
    "- **Spaces** â€” interactive demos and apps\n",
    "\n",
    "We'll use the `datasets` library to load **nfcorpus**, a biomedical information retrieval dataset from the [BEIR benchmark](https://github.com/beir-cellar/beir). It contains:\n",
    "- A **corpus** of 3,633 biomedical documents (titles + abstracts)\n",
    "- **Queries** â€” 3,237 plain-English health/nutrition queries\n",
    "- **Relevance judgments** (qrels) â€” 12,334 human-labeled query-to-document mappings\n",
    "\n",
    "Tips:\n",
    "- Browse datasets at [huggingface.co/datasets](https://huggingface.co/datasets)\n",
    "- The `datasets` library handles downloading, caching, and format conversion\n",
    "- Datasets have **configurations** (sub -datasets) and **splits** (train/test/etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the nfcorpus dataset from Hugging Face\n",
    "# It has separate \"corpus\" and \"queries\" configurations (like sub-datasets)\n",
    "corpus = load_dataset(\"BeIR/nfcorpus\", \"corpus\", split=\"corpus\")\n",
    "queries = load_dataset(\"BeIR/nfcorpus\", \"queries\", split=\"queries\")\n",
    "\n",
    "# Relevance judgments (qrels) are stored in a separate dataset\n",
    "# We use the \"test\" split â€” these are the queries we'll evaluate on\n",
    "qrels = load_dataset(\"BeIR/nfcorpus-qrels\", split=\"test\")\n",
    "\n",
    "print(f\"Documents: {len(corpus)}\")\n",
    "print(f\"Queries:   {len(queries)}\")\n",
    "print(f\"Relevance judgments: {len(qrels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corpus_df = corpus.to_pandas()\n",
    "queries_df = queries.to_pandas()\n",
    "qrels_df = qrels.to_pandas()\n",
    "\n",
    "# Remove some duplicate documents (same title+text, different IDs)\n",
    "n_before = len(corpus_df)\n",
    "corpus_df = corpus_df.drop_duplicates(subset=[\"title\", \"text\"], keep=\"first\")\n",
    "print(f\"Deduplicated corpus: {n_before} -> {len(corpus_df)} documents\")\n",
    "\n",
    "# Inspect the column structure of each DataFrame\n",
    "print(f\"\\nCorpus columns:  {list(corpus_df.columns)}\")\n",
    "print(f\"Queries columns: {list(queries_df.columns)}\")\n",
    "print(f\"Qrels columns:   {list(qrels_df.columns)}\")\n",
    "\n",
    "# Show sample documents â€” each has an ID, title, and full text (abstract)\n",
    "print(\"\\n--- Sample documents ---\")\n",
    "for _, row in corpus_df.head(3).iterrows():\n",
    "    print(f\"ID:    {row['_id']}\")\n",
    "    print(f\"Title: {row['title']}\")\n",
    "    print(f\"Text:  {row['text']}...\")\n",
    "    print()\n",
    "\n",
    "# Show sample queries â€” plain-English health/nutrition questions\n",
    "print(\"--- Sample queries ---\")\n",
    "for _, row in queries_df.head(5).iterrows():\n",
    "    print(f\"ID:    {row['_id']}\")\n",
    "    print(f\"Text:  {row['text']}\")\n",
    "    print()\n",
    "\n",
    "# Show sample relevance judgments â€” each row links a query to a document with a score\n",
    "print(\"\\n--- Sample relevance judgments ---\")\n",
    "print(qrels_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Explore the Data\n",
    "\n",
    "Before building anything, **look at your data**.\n",
    "\n",
    "We have three DataFrames:\n",
    "- `corpus_df` â€” 3,633 documents with `_id`, `title`, and `text`\n",
    "- `queries_df` â€” 3,237 queries with `_id`, `title`, and `text`\n",
    "- `qrels_df` â€” 12,334 relevance judgments linking queries to documents with a `score` (1 = relevant, 2 = highly relevant)\n",
    "\n",
    "This dataset has one amazing feature: it comes with **human-labeled ground truth** â€” we know exactly which documents are relevant to which queries. This is the gold standard for evaluating retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Document statistics ---\n",
    "print(\"Document text length (characters):\")\n",
    "print(corpus_df[\"text\"].str.len().describe().round(0))\n",
    "print()\n",
    "\n",
    "# Relevance scores: 1 = relevant, 2 = highly relevant\n",
    "print(\"Relevance score distribution (1 = relevant, 2 = highly relevant):\")\n",
    "print(qrels_df[\"score\"].value_counts().sort_index().to_string())\n",
    "print()\n",
    "\n",
    "# Not all queries have ground truth â€” only 323 of 3,237 have qrels\n",
    "n_queries_with_qrels = qrels_df[\"query-id\"].nunique()\n",
    "print(f\"Queries with relevance judgments: {n_queries_with_qrels} (out of {len(queries_df)})\")\n",
    "\n",
    "# How many relevant docs per query? This affects recall interpretation later.\n",
    "# If a query has 50 relevant docs but we only retrieve 20, recall can't exceed 0.4\n",
    "per_query = qrels_df.groupby(\"query-id\")[\"corpus-id\"].count()\n",
    "print(\"\\nRelevant docs per query:\")\n",
    "print(per_query.describe().round(1))\n",
    "\n",
    "# --- Example: look at one query and all its relevant documents ---\n",
    "# Build a quick lookup by doc ID for the full corpus (before dedup)\n",
    "corpus_lookup = corpus.to_pandas().set_index(\"_id\")\n",
    "\n",
    "sample_qid = qrels_df[\"query-id\"].iloc[0]\n",
    "sample_rels = qrels_df[qrels_df[\"query-id\"] == sample_qid]\n",
    "q_row = queries_df[queries_df[\"_id\"] == sample_qid].iloc[0]\n",
    "print(f\"\\nExample query [{sample_qid}]: {q_row['text']}\")\n",
    "print(f\"  Has {len(sample_rels)} relevant documents:\")\n",
    "for _, r in sample_rels.iterrows():\n",
    "    doc = corpus_lookup.loc[r[\"corpus-id\"]]\n",
    "    label = \"highly relevant\" if r[\"score\"] == 2 else \"relevant\"\n",
    "    print(f\"    ({label}) {doc['title'][:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Set Up Search with LanceDB\n",
    "\n",
    "> ðŸ“š **TERM: Vector Database**  \n",
    "> A database optimized for storing and searching over embeddings (vectors). Instead of exact keyword matching, vector databases find items that are *semantically similar* to a query.\n",
    "\n",
    "> ðŸ“š **TERM: LanceDB**  \n",
    "> An open-source, embedded vector database. It runs locally (no server, no account needed), handles embeddings automatically, and supports vector search, lexical search, and hybrid search.\n",
    "\n",
    "LanceDB uses Pydantic-type models to define your table schema\n",
    "- `SourceField()` â€” tells LanceDB which column to embed\n",
    "- `VectorField()` â€” tells LanceDB where to store the embedding vector\n",
    "\n",
    "LanceDB can use OpenAI's embedding API automatically through its **registry**. We pick an embedding model, define our schema, and LanceDB handles the rest.\n",
    "\n",
    "LanceDB supports three search modes:\n",
    "- `\"vector\"` â€” embedding-based semantic search (like what you built with cosine similarity)\n",
    "- `\"fts\"`    â€” **full-text search**, i.e. lexical search (like the BM25 you built from scratch). \n",
    "- `\"hybrid\"` â€” combines both vector and lexical search\n",
    "\n",
    "We'll concatenate each document's title and text into a single `content` field for both embedding and search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import lancedb\n",
    "from lancedb.embeddings import get_registry\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "\n",
    "# Set up the embedding function â€” LanceDB will call OpenAI's API automatically\n",
    "func = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n",
    "\n",
    "# Define the table schema\n",
    "# - SourceField() marks the column to embed\n",
    "# - VectorField() stores the resulting vector\n",
    "class Document(LanceModel):\n",
    "    doc_id: str\n",
    "    title: str\n",
    "    content: str = func.SourceField()  # this column gets embedded automatically\n",
    "    vector: Vector(func.ndims()) = func.VectorField()  # embedding stored here\n",
    "\n",
    "\n",
    "# Create a local LanceDB database and table\n",
    "db = lancedb.connect(\"../temp/lancedb\")\n",
    "table = db.create_table(\"nfcorpus\", schema=Document, mode=\"overwrite\")\n",
    "\n",
    "# Combine title + text into a single content field for richer embeddings\n",
    "data = [\n",
    "    {\"doc_id\": row[\"_id\"], \"title\": row[\"title\"], \"content\": row[\"title\"] + \"\\n\" + row[\"text\"]}\n",
    "    for _, row in corpus_df.iterrows()\n",
    "]\n",
    "\n",
    "# Add documents in batches\n",
    "batch_size = 500\n",
    "for i in range(0, len(data), batch_size):\n",
    "    table.add(data[i : i + batch_size])\n",
    "    print(f\"  Embedded {min(i + batch_size, len(data))}/{len(data)} documents\")\n",
    "\n",
    "# Build a full-text search (FTS) index for lexical/BM25 search\n",
    "table.create_fts_index(\"content\", replace=True)\n",
    "\n",
    "print(f\"\\n{table.count_rows()} documents indexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "We will now test the three search modes on some sample queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all three search modes on a sample query\n",
    "query = \"How to prevent heart disease\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for mode in [\"vector\", \"fts\", \"hybrid\"]:\n",
    "    results = table.search(query, query_type=mode).limit(3).to_list()\n",
    "    print(f\"\\n--- {mode} search ---\")\n",
    "    for i, r in enumerate(results):\n",
    "        print(f\"  [{i + 1}] {r['title'][:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Retrieval Metrics\n",
    "\n",
    "Now that we have a search system, how do we measure how well it's working? We need **retrieval metrics**.\n",
    "\n",
    "> ðŸ“š **TERM: Precision@k**  \n",
    "> Of the items you retrieved in the top *k*, what fraction are actually relevant?\n",
    "> $$\\text{Precision@k} = \\frac{\\text{\\# relevant items in top-k}}{k}$$\n",
    "\n",
    "> ðŸ“š **TERM: Recall@k**  \n",
    "> Out of all the relevant items that exist, what fraction did you find in the top *k* results?\n",
    "> $$\\text{Recall@k} = \\frac{\\text{\\# relevant items in top-k}}{\\text{total \\# relevant items}}$$\n",
    "\n",
    "As k increases, \n",
    "- recall goes up -- you find more relevant items\n",
    "- precision tends to drop -- you pull in more irrelevant items\n",
    "\n",
    "\n",
    "Nfcorpus comes with **human relevance judgments** (qrels). For each test query, we know exactly which documents are relevant. This allows us to compute clean, reliable metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a lookup: for each query, the set of relevant document IDs\n",
    "qrels_by_query = qrels_df.groupby(\"query-id\")[\"corpus-id\"].apply(set).to_dict()\n",
    "\n",
    "# Only evaluate queries that have ground truth relevance judgments\n",
    "test_query_ids = list(qrels_by_query.keys())\n",
    "test_queries = queries_df[queries_df[\"_id\"].isin(test_query_ids)]\n",
    "print(f\"Evaluating on {len(test_queries)} queries with ground truth\\n\")\n",
    "\n",
    "# We'll compute metrics at multiple values of k\n",
    "k_values = [1, 3, 5, 10, 20]\n",
    "max_k = 20\n",
    "\n",
    "# Store results in tidy format: one row per (query, metric, k) combination\n",
    "tidy_rows = []\n",
    "\n",
    "for _, row in test_queries.iterrows():\n",
    "    query_id = row[\"_id\"]\n",
    "    query_text = row[\"text\"]\n",
    "    relevant_ids = qrels_by_query[query_id]\n",
    "\n",
    "    # Retrieve top-k documents using vector search\n",
    "    retrieved = table.search(query_text, query_type=\"vector\").limit(max_k).to_list()\n",
    "    retrieved_ids = [r[\"doc_id\"] for r in retrieved]\n",
    "\n",
    "    for k in k_values:\n",
    "        # Get the set of doc IDs in the top-k results\n",
    "        ids_at_k = set(retrieved_ids[:k])\n",
    "        # Count how many of the top-k are actually relevant\n",
    "        n_relevant_at_k = len(ids_at_k & relevant_ids)\n",
    "\n",
    "        # Precision@k = fraction of retrieved docs that are relevant\n",
    "        precision = n_relevant_at_k / k\n",
    "        # Recall@k = fraction of all relevant docs that we found\n",
    "        recall = n_relevant_at_k / len(relevant_ids)\n",
    "\n",
    "        tidy_rows.append({\"metric\": \"precision\", \"k\": k, \"search_type\": \"vector\", \"score\": precision, \"query_id\": query_id})\n",
    "        tidy_rows.append({\"metric\": \"recall\", \"k\": k, \"search_type\": \"vector\", \"score\": recall, \"query_id\": query_id})\n",
    "\n",
    "eval_df = pd.DataFrame(tidy_rows)\n",
    "\n",
    "# Show average precision and recall at each k\n",
    "print(\"Retrieval evaluation (vector search, ground truth qrels):\\n\")\n",
    "print(eval_df.groupby([\"search_type\", \"metric\", \"k\"])[\"score\"].mean().round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Synthetic Question Generation\n",
    "\n",
    "Nfcorpus comes with human relevance judgments, but most real-world datasets don't. When you build a RAG system over your company's docs -- or a new AI system more generally -- you will often have no ground truth.\n",
    "\n",
    "One workaround for this probem is to **generate synthetic data**. The idea is simple:\n",
    "1. Pick a document from your corpus\n",
    "2. Ask an LLM to generate a question that this document can answer\n",
    "3. Now you have a (question, document_id) pair with **known ground truth**\n",
    "\n",
    "Because we know exactly which document the question came from, we can compute retrieval metrics: did the search system return the source document in its top-k?\n",
    "\n",
    "**Diversifying our synthetic data** If you use the same prompt every time, you'll get repetitive questions. One trick is to add randomly some prompt \"constraints\" to force variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "import asyncio\n",
    "import random\n",
    "import textwrap\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# 'chain_of_thought' makes the LLM reason before generating the question\n",
    "class SyntheticQuestion(BaseModel):\n",
    "    chain_of_thought: str = Field(description=\"Step-by-step reasoning about what makes a good question for this document\")\n",
    "    question: str = Field(description=\"A natural, specific question that can be answered using the document\")\n",
    "    answer: str = Field(description=\"The answer to the question\")\n",
    "\n",
    "\n",
    "# random \"constraints\" \n",
    "constraints = [\n",
    "    \"The question should be answerable in one word or a short phrase\",\n",
    "    \"The question should require synthesizing multiple facts from the document\",\n",
    "    \"Frame the question as something a patient might ask their doctor\",\n",
    "    \"Ask about a specific number, date, or finding mentioned in the document\",\n",
    "]\n",
    "\n",
    "\n",
    "async def generate_question(doc_id: str, title: str, text: str) -> dict:\n",
    "    \"\"\"Generate a synthetic question for a single document using an LLM.\"\"\"\n",
    "    constraint = random.choice(constraints)\n",
    "    response = await litellm.acompletion(\n",
    "        model=\"gpt-5.1\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": textwrap.dedent(f\"\"\"\n",
    "                \n",
    "                I will give you a document from BEIR's nfcorpus -- a dataset that has a collection of biomedical documents. Please Generate a question that can be answered using the following document.\n",
    "                \n",
    "                Title: {title}\n",
    "                Text: {text}\n",
    "                \n",
    "                Rules:\n",
    "                - Your question should be natural and specific and concise\n",
    "                - Your question should not assume that someone is reading the document, but rather that they are asking a general biomedical questions\n",
    "                - Your question must be answerable using the document that I gave you\n",
    "                - {constraint}\n",
    "                - Do not reference \\\"the document\\\" or \\\"the study\\\" in your question\n",
    "                \"\"\"\n",
    "                ),\n",
    "            }\n",
    "        ],\n",
    "        response_format=SyntheticQuestion,\n",
    "    )\n",
    "    \n",
    "    # Parse the JSON response into our Pydantic model\n",
    "    result = SyntheticQuestion.model_validate_json(response.choices[0].message.content)\n",
    "    return {\"doc_id\": doc_id, \"question\": result.question, \"answer\": result.answer}\n",
    "\n",
    "\n",
    "# Sample 80 documents to generate questions for\n",
    "sample_docs = corpus_df.sample(n=80, random_state=42)\n",
    "\n",
    "# Generate all questions concurrently using asyncio.gather\n",
    "tasks = [generate_question(row[\"_id\"], row[\"title\"], row[\"text\"]) for _, row in sample_docs.iterrows()]\n",
    "synthetic_results = await asyncio.gather(*tasks)\n",
    "\n",
    "synthetic_df = pd.DataFrame(synthetic_results)\n",
    "print(f\"Generated {len(synthetic_df)} synthetic questions\\n\")\n",
    "\n",
    "# Show some examples with their source documents\n",
    "for _, row in synthetic_df.head(5).iterrows():\n",
    "    doc = corpus_df[corpus_df[\"_id\"] == row[\"doc_id\"]].iloc[0]\n",
    "    print(f\"Q: {row['question']}\")\n",
    "    print(f\"   Source: {doc['title'][:80]}\")\n",
    "    print(f\"   Answer: {row['answer']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate retrieval on synthetic questions\n",
    "# Key difference from qrels: each synthetic question has exactly 1 relevant doc (its source)\n",
    "syn_rows = []\n",
    "\n",
    "for _, row in synthetic_df.iterrows():\n",
    "    source_id = row[\"doc_id\"]\n",
    "    question = row[\"question\"]\n",
    "\n",
    "    # Search for the synthetic question\n",
    "    retrieved = table.search(question, query_type=\"vector\").limit(max_k).to_list()\n",
    "    retrieved_ids = [r[\"doc_id\"] for r in retrieved]\n",
    "\n",
    "    for k in k_values:\n",
    "        ids_at_k = retrieved_ids[:k]\n",
    "\n",
    "        # Binary relevance: did we find the source document in top-k?\n",
    "        found = source_id in ids_at_k\n",
    "        precision = (1.0 if found else 0.0) / k  # at most 1 relevant doc\n",
    "        recall = 1.0 if found else 0.0  # found it or didn't\n",
    "\n",
    "        syn_rows.append({\"metric\": \"precision\", \"k\": k, \"search_type\": \"vector\", \"score\": precision, \"question\": question})\n",
    "        syn_rows.append({\"metric\": \"recall\", \"k\": k, \"search_type\": \"vector\", \"score\": recall, \"question\": question})\n",
    "\n",
    "syn_eval_df = pd.DataFrame(syn_rows)\n",
    "\n",
    "# Compare with qrels results above â€” synthetic questions are typically \"easier\" for retrieval\n",
    "# because the LLM generates questions using the document's own language\n",
    "print(\"Retrieval evaluation (synthetic questions, vector search):\\n\")\n",
    "print(syn_eval_df.groupby([\"search_type\", \"metric\", \"k\"])[\"score\"].mean().round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "**Synthetic vs. human-labeled evaluation**: Notice the difference â€” synthetic questions give much higher precision than human qrels. Why? Because the LLM generates questions using the document's own vocabulary, making them easier to retrieve via embedding similarity. Real user queries are messier and more diverse. This is an important caveat: **synthetic evals can overestimate your system's real-world performance**. They're great when you have no ground truth at all, but treat the numbers as an upper bound."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Having said that we can make much more difficult synthetic questions..\n",
    "\n",
    "**Question: How would you do that?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Experiment and Improve\n",
    "\n",
    "Let's use our ground truth qrels to compare different search strategies. We'll run vector, full-text, and hybrid search and organize results in **tidy data format** â€” each row is one observation.\n",
    "\n",
    "**A note on recall**: as we saw in Section 2, queries in nfcorpus have a **median of 16 relevant documents** (and a mean of 38). Since we only retrieve up to k=20, we can't possibly find them all â€” so recall values will be low. This is expected and not a problem with our search system. In practice, **precision tells us about the quality of our top results** (are the docs we return actually useful?), while recall tells us how much of the total relevant information we're capturing. Both matter, but for RAG â€” where we feed a handful of documents to an LLM â€” precision is usually more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compare all three search strategies on the same 323 test queries\n",
    "search_types = [\"vector\", \"fts\", \"hybrid\"]\n",
    "experiment_rows = []\n",
    "\n",
    "for search_type in search_types:\n",
    "    print(f\"Evaluating {search_type} search...\")\n",
    "    for _, row in test_queries.iterrows():\n",
    "        query_id = row[\"_id\"]\n",
    "        query_text = row[\"text\"]\n",
    "        relevant_ids = qrels_by_query[query_id]\n",
    "\n",
    "        # Retrieve top-k documents using this search strategy\n",
    "        retrieved = table.search(query_text, query_type=search_type).limit(max_k).to_list()\n",
    "        retrieved_ids = [r[\"doc_id\"] for r in retrieved]\n",
    "\n",
    "        # Compute precision and recall at each k\n",
    "        for k in k_values:\n",
    "            ids_at_k = set(retrieved_ids[:k])\n",
    "            n_relevant_at_k = len(ids_at_k & relevant_ids)\n",
    "\n",
    "            precision = n_relevant_at_k / k\n",
    "            recall = n_relevant_at_k / len(relevant_ids)\n",
    "\n",
    "            experiment_rows.append(\n",
    "                {\"metric\": \"precision\", \"k\": k, \"search_type\": search_type, \"score\": precision, \"query_id\": query_id}\n",
    "            )\n",
    "            experiment_rows.append(\n",
    "                {\"metric\": \"recall\", \"k\": k, \"search_type\": search_type, \"score\": recall, \"query_id\": query_id}\n",
    "            )\n",
    "\n",
    "# Tidy data format: each row is one observation (metric, k, search_type, score, query_id)\n",
    "experiment_df = pd.DataFrame(experiment_rows)\n",
    "\n",
    "# Show average scores across all queries\n",
    "print(\"\\nResults:\\n\")\n",
    "print(experiment_df.groupby([\"search_type\", \"metric\", \"k\"])[\"score\"].mean().round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision@k and recall@k curves side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: precision@k â€” how precise are the top results?\n",
    "for search_type in search_types:\n",
    "    data = experiment_df[(experiment_df[\"metric\"] == \"precision\") & (experiment_df[\"search_type\"] == search_type)]\n",
    "    means = data.groupby(\"k\")[\"score\"].mean()\n",
    "    ax1.plot(means.index, means.values, marker=\"o\", label=search_type)\n",
    "\n",
    "ax1.set_xlabel(\"k\")\n",
    "ax1.set_ylabel(\"Precision@k\")\n",
    "ax1.set_title(\"Precision@k by Search Type\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "# Right plot: recall@k â€” what fraction of relevant docs did we find?\n",
    "# Recall is low because queries have many relevant docs (median 16, mean 38)\n",
    "# but we only retrieve up to k=20\n",
    "for search_type in search_types:\n",
    "    data = experiment_df[(experiment_df[\"metric\"] == \"recall\") & (experiment_df[\"search_type\"] == search_type)]\n",
    "    means = data.groupby(\"k\")[\"score\"].mean()\n",
    "    ax2.plot(means.index, means.values, marker=\"o\", label=search_type)\n",
    "\n",
    "ax2.set_xlabel(\"k\")\n",
    "ax2.set_ylabel(\"Recall@k\")\n",
    "ax2.set_title(\"Recall@k by Search Type\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "ax2.set_ylim(0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "| What we learned | Key takeaway |\n",
    "|---|---|\n",
    "| **Hugging Face** | A one-stop ecosystem for datasets and models. The `datasets` library makes loading and caching easy. |\n",
    "| **LanceDB** | An embedded vector database â€” no server, no account. Define a Pydantic schema and it handles embeddings + search for you. |\n",
    "| **Precision & Recall** | Precision@k measures the quality of your top results. Recall@k measures how much relevant information you're capturing. Both matter. |\n",
    "| **Synthetic questions** | When you don't have ground truth, generate test questions with LLMs. Useful but can overestimate real-world performance. |\n",
    "| **Experiments** | Compare strategies (vector, lexical, hybrid) on the same queries with the same metrics. Let the data tell you what works. |\n",
    "\n",
    "**The big picture**: Evaluations turn \"I think this works\" into \"I measured this and it works.\" Every time you change your retrieval strategy, embedding model, chunking approach, or prompt â€” run your evals and compare. That's how you improve systematically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ce)",
   "language": "python",
   "name": "ce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
